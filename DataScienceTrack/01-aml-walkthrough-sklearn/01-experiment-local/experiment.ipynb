{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting in a local environment\n",
    "In this lab you will experiment with a couple of machine learning algorithms using a small development dataset and local storage and compute. You will use Azure Machine Learning service to track your training runs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to AML workspace\n",
    "To connect to the workspace created during the lab setup load the file **config.json**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /home/demouser/repos/MTC_AzureAILabs/DataScienceTrack/01-aml-walkthrough-sklearn/aml_config/config.json\n",
      "jkaml1\n",
      "jkaml1\n",
      "eastus2\n",
      "952a710c-8d9c-40c1-9fec-f752138cc0b3\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment\n",
    "\n",
    "You will track a record of training runs in AML Experiment. A workspace can have multiple experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'aerial-experiment'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dowload the development dataset\n",
    "\n",
    "You will experiment using a small development dataset. In many cases, your local development environment will not have enough computational and storage resources to support experimentation on full datasets. A common machine learning workflow pattern is to develop and debug your training scripts in a local development environment and then run training jobs on full datasets using powerfull cloud compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-28 01:36:52 URL:https://azureailabs.blob.core.windows.net/aerialtar/aerialtiny.tar.gz [103131426/103131426] -> \"aerialtiny.tar.gz\" [1]\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "wget -nv https://azureailabs.blob.core.windows.net/aerialtar/aerialtiny.tar.gz\n",
    "tar -xf aerialtiny.tar.gz\n",
    "ls -l aerialtiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 104\n",
      "drwxrwxr-x 2 demouser demouser 20480 Oct  6 17:46 Barren\n",
      "drwxrwxr-x 2 demouser demouser 16384 Oct  6 17:46 Cultivated\n",
      "drwxrwxr-x 2 demouser demouser 16384 Oct  6 17:46 Developed\n",
      "drwxrwxr-x 2 demouser demouser 16384 Oct  6 17:46 Forest\n",
      "drwxrwxr-x 2 demouser demouser 20480 Oct  6 17:46 Herbaceous\n",
      "drwxrwxr-x 2 demouser demouser 16384 Oct  6 17:46 Shrub\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls -l aerialtiny/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the development dataset\n",
    "\n",
    "Load the training and validation datasets to `numpy` arrays. The images are in `PNG` format. The size is `(224, 224, 3)` and the color encoding is `RGB`\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1060, 224, 224, 3)\n",
      "(1060,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "\n",
    "# Define a utility function to load images from a folder\n",
    "def load_images(input_dir):\n",
    "    label_to_integer = {\n",
    "        \"Barren\": 0,\n",
    "        \"Cultivated\": 1,\n",
    "        \"Developed\": 2,\n",
    "        \"Forest\": 3,\n",
    "        \"Herbaceous\": 4,\n",
    "        \"Shrub\": 5}\n",
    "    \n",
    "    images = [(imread(os.path.join(input_dir, folder, filename)), label_to_integer[folder])\n",
    "             for folder in os.listdir(input_dir)\n",
    "             for filename in os.listdir(os.path.join(input_dir, folder))]\n",
    "    \n",
    "    images, labels = zip(*images)\n",
    "    \n",
    "    return np.asarray(images), np.asarray(labels)\n",
    "\n",
    "\n",
    "# Load training images\n",
    "training_images_dir = 'aerialtiny/train'\n",
    "training_images, training_labels = load_images(training_images_dir)\n",
    "\n",
    "# Load validation images\n",
    "validate_images_dir = 'aerialtiny/valid'\n",
    "validation_images, validation_labels = load_images(validation_images_dir)\n",
    "\n",
    "print(training_images.shape)\n",
    "print(training_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a local model\n",
    "Train a simple logistic regression model using `sckit-learn`. Since our images are in (224, 224,3 ) RGB format we need to flatten them to conform to logistic regression input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:\n",
      "  Training images:  (1060, 150528)\n",
      "  Training labels:  (1060,)\n",
      "Starting training ...\n",
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demouser/anaconda3/envs/aml/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:718: UserWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"lbfgs failed to converge. Increase the number \"\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.5min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Reshape training and validation datasets\n",
    "X_train = np.ndarray.reshape(training_images, (training_images.shape[0], -1))\n",
    "X_validate = np.ndarray.reshape(validation_images, (validation_images.shape[0], -1))\n",
    "y_train = training_labels\n",
    "y_valid = validation_labels\n",
    "\n",
    "# Print the shape of inputs\n",
    "print(\"Input data:\")\n",
    "print(\"  Training images: \", X_train.shape)\n",
    "print(\"  Training labels: \", y_train.shape)\n",
    "\n",
    "# Train logistic regression\n",
    "print(\"Starting training ...\")\n",
    "lr = LogisticRegression(\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    verbose=1)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate performance measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.3978494623655914\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Run the model on validation images\n",
    "y_hat = lr.predict(X_validate)\n",
    "\n",
    "# Display accuracy\n",
    "print(\"Validation accuracy:\", np.average(y_hat == y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated by the validation accuracy, our model's performance is rather absymal. Logistic regression can only learn linear decision boundries. Let's try an ML algorithm with more capacity - Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training ...\n",
      "Training completed.\n",
      "Validation accuracy: 0.6827956989247311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   11.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train logistic regression\n",
    "print(\"Starting training ...\")\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=7,\n",
    "    verbose=1)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"Training completed.\")\n",
    "\n",
    "# Run the model on validation images\n",
    "y_hat = rf.predict(X_validate)\n",
    "\n",
    "# Display accuracy\n",
    "print(\"Validation accuracy:\", np.average(y_hat == y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better than logist regression but still pretty bad. We could attempt to fine-tune hyper-parameters or try other machine learning algorithms but the experience of the last few decades of machine learning research teaches us that barring applying sophisticated feature engineering, \"classical\" machine learning algorithms don't perform well on tasks like non-trivial image classification. Rather than pursuing this approach you will apply a proven technique that has emerged in the recent years - Transfer Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "In the next lab you will utilize a pre-trained deep neural network to extract powerful features from images and use them to train a better performing classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
