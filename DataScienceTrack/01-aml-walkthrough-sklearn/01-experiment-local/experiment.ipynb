{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting in a local environment\n",
    "In this lab you will experiment with a couple of machine learning algorithms using a small development dataset and local storage and compute. In many cases, your local development environment will not have enough computational and storage resources to support training on full datasets. A common machine learning workflow pattern is to develop and debug your training scripts in a local development environment and then run training jobs on full datasets using powerfull cloud compute resources.\n",
    "\n",
    "You will use Azure Machine Learning service to track your training runs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dowload the development dataset\n",
    "\n",
    "The datasets used in the labs have been uploaded to a public container in Azure Blob Storage.\n",
    "\n",
    "Download the small development dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12\n",
      "drwxrwxr-x 8 demouser demouser 4096 Oct  6 17:45 test\n",
      "drwxrwxr-x 8 demouser demouser 4096 Oct  6 17:45 train\n",
      "drwxrwxr-x 8 demouser demouser 4096 Oct  6 17:45 valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-31 01:29:07 URL:https://azureailabs.blob.core.windows.net/aerialtar/aerialtiny.tar.gz [103131426/103131426] -> \"aerialtiny.tar.gz\" [1]\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "wget -nv https://azureailabs.blob.core.windows.net/aerialtar/aerialtiny.tar.gz\n",
    "tar -xf aerialtiny.tar.gz\n",
    "ls -l aerialtiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 116\n",
      "drwxrwxr-x 2 demouser demouser 20480 Oct  6 17:46 Barren\n",
      "drwxrwxr-x 2 demouser demouser 20480 Oct  6 17:46 Cultivated\n",
      "drwxrwxr-x 2 demouser demouser 20480 Oct  6 17:46 Developed\n",
      "drwxrwxr-x 2 demouser demouser 16384 Oct  6 17:46 Forest\n",
      "drwxrwxr-x 2 demouser demouser 20480 Oct  6 17:46 Herbaceous\n",
      "drwxrwxr-x 2 demouser demouser 20480 Oct  6 17:46 Shrub\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls -l aerialtiny/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess images\n",
    "\n",
    "Load the training and validation datasets to `numpy` arrays. The images are in `PNG` format. The size is `(224, 224, 3)` and the color encoding is `RGB`\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-21c3aa78596a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define a utility function to load images from a folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "\n",
    "# Define a utility function to load images from a folder\n",
    "def load_images(input_dir):\n",
    "    label_to_integer = {\n",
    "        \"Barren\": 0,\n",
    "        \"Cultivated\": 1,\n",
    "        \"Developed\": 2,\n",
    "        \"Forest\": 3,\n",
    "        \"Herbaceous\": 4,\n",
    "        \"Shrub\": 5}\n",
    "    \n",
    "    images = [(imread(os.path.join(input_dir, folder, filename)), label_to_integer[folder])\n",
    "             for folder in os.listdir(input_dir)\n",
    "             for filename in os.listdir(os.path.join(input_dir, folder))]\n",
    "    \n",
    "    images, labels = zip(*images)\n",
    "    \n",
    "    return np.asarray(images), np.asarray(labels)\n",
    "\n",
    "\n",
    "# Load training images\n",
    "training_images_dir = 'aerialtiny/train'\n",
    "training_images, training_labels = load_images(training_images_dir)\n",
    "\n",
    "# Load validation images\n",
    "validation_images_dir = 'aerialtiny/valid'\n",
    "validation_images, validation_labels = load_images(validation_images_dir)\n",
    "\n",
    "print(training_images.shape)\n",
    "print(training_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a local model\n",
    "In this step you will train a logistic regression model from `sckit-learn`, directly on image pixel data. This is a little bit of a naive approach as experience teaches us that simple machine learning models don't perform well on raw image data unless dealing with really simplistic scenarios like the MNIST dataset. Nevertheleess, we will use this approach to demostrate how to track the training progress using AML Experiment and Run entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to AML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /data/home/demouser/notebooks/MTC_AzureAILabs/DataScienceTrack/01-aml-walkthrough-sklearn/aml_config/config.json\n",
      "jkamllab\n",
      "jkamllab\n",
      "eastus2\n",
      "952a710c-8d9c-40c1-9fec-f752138cc0b3\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess images\n",
    "Since our images are in (224, 224,3 ) RGB format we need to flatten them to conform to the input shape required by logistic regression and other sckit-learn ml algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-54ca435aa369>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reshape training and validation datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_validate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalidation_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Reshape training and validation datasets\n",
    "X_train = np.ndarray.reshape(training_images, (training_images.shape[0], -1))\n",
    "X_validate = np.ndarray.reshape(validation_images, (validation_images.shape[0], -1))\n",
    "y_train = training_labels\n",
    "y_valid = validation_labels\n",
    "\n",
    "# Print the shape of inputs\n",
    "print(\"Input data:\")\n",
    "print(\"  Training images: \", X_train.shape)\n",
    "print(\"  Training labels: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a model\n",
    "We will track the model's hyper-parameters, performance, and serialized model file in AML Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "# Create AML Experiment\n",
    "experiment_name = 'aerial-train-in-notebook'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "# Initialize logging\n",
    "run = exp.start_logging()\n",
    "\n",
    "# Log run description and hyper-parameter values\n",
    "run.tag(\"Description\", \"Naive attempt to fit logistic regression to aerial image data\")\n",
    "run.log(\"Solver\", \"lbfgs\")\n",
    "run.log(\"C\", 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Train logistic regression\n",
    "print(\"Starting training ...\")\n",
    "lr = LogisticRegression(\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    C = 1.0,\n",
    "    verbose=1)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Training completed.\")\n",
    "\n",
    "# Evaluate the model on validation images\n",
    "print(\"Starting evaluation\")\n",
    "y_hat = lr.predict(X_validate)\n",
    "val_accuracy = np.average(y_hat == y_valid)\n",
    "print(\"Validation accuracy:\", val_accuracy)\n",
    "run.log('Validation accuracy', val_accuracy)\n",
    "\n",
    "# Save and upload the model\n",
    "joblib.dump(value=lr, filename='model.pkl')\n",
    "run.upload_file(name='outputs/model.pkl', path_or_stream='./model.pkl')\n",
    "\n",
    "# Finalize the run\n",
    "run.complete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can browse the recorded run in Azure portal.\n",
    "\n",
    "As shown by the validation accuracy, our model's performance is rather absymal. Logistic regression can only learn linear decision boundries. Let's try an ML algorithm with more capacity - Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize logging\n",
    "run = exp.start_logging()\n",
    "\n",
    "# Log run description and hyper-parameter values\n",
    "run.tag(\"Description\", \"Another naive attempt to train on aerial image data - random forests\")\n",
    "run.log(\"No of trees\", 100)\n",
    "run.log(\"Max Depth\", 7)\n",
    "\n",
    "# Train logistic regression\n",
    "print(\"Starting training ...\")\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=7,\n",
    "    verbose=1)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"Training completed.\")\n",
    "\n",
    "# Evaluate the model on validation images\n",
    "print(\"Starting evaluation\")\n",
    "y_hat = rf.predict(X_validate)\n",
    "val_accuracy = np.average(y_hat == y_valid)\n",
    "print(\"Validation accuracy:\", val_accuracy)\n",
    "run.log('Validation accuracy', val_accuracy)\n",
    "\n",
    "# Save and upload the model\n",
    "joblib.dump(value=lr, filename='model.pkl')\n",
    "run.upload_file(name='outputs/model.pkl', path_or_stream='./model.pkl')\n",
    "\n",
    "# Finalize the run\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better than logistic regression but still pretty bad. We could attempt to fine-tune hyper-parameters or try other machine learning algorithms but rather thank pursuing the approach of training a model on \"raw\" image data  we apply a proven technique that has emerged in the recent years - Transfer Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "In the next lab you will utilize a pre-trained deep neural network to extract powerful features from images and use them to train a better performing classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
