{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Model training and evaluation\n",
    "In this lab we will train a multinomial classification model using the bottleneck features created in Lab 2.\n",
    "\n",
    "\n",
    "![Transfer Learning](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/tlcl.png)\n",
    "\n",
    "We will start with training a simple logistic regression model in a local environment to validate that the bottleneck features can improve our classifier. We will then use the Azure ML feature called `Automated ML` to find the most optimal model for our image classification task.\n",
    "\n",
    "![AML Arch](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/automated-machine-learning.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to AML workspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "import os\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train logistic regression model locally\n",
    "\n",
    "We will start by training a logistic regression model locally using the bottleneck features created in Lab 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download bottleneck features\n",
    "\n",
    "In the previous lab, the bottleneck features have been uploaded to the `bottleneck_features`folder in the default datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'bottleneck_features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "ds.download(target_path='.', prefix=data_folder, overwrite=True)\n",
    "!ls bottleneck_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train logistic regression using bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load features and labels into numpy arrays\n",
    "file_name = os.path.join(data_folder, 'aerial_bottleneck_resnet50.h5')\n",
    "with h5py.File(file_name, \"r\") as hfile:\n",
    "    features = np.array(hfile.get('features'))\n",
    "    labels = np.array(hfile.get('labels'))\n",
    " \n",
    "# Split the data into training and validation partitions   \n",
    "X_train, X_validation, y_train, y_validation = train_test_split(features, labels,\n",
    "                                                               test_size=0.1,\n",
    "                                                               shuffle=True,\n",
    "                                                               stratify=labels)\n",
    "    \n",
    "# Train logistics regresssion model\n",
    "reg = 0.1\n",
    "clf = LogisticRegression(\n",
    "        C=1.0/reg, \n",
    "        multi_class='multinomial',\n",
    "        solver='lbfgs',\n",
    "        random_state=1)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate\n",
    "y_hat = clf.predict(X_validation)\n",
    "    \n",
    "# Calculate accuracy \n",
    "acc = np.average(y_hat == y_validation)\n",
    "print('Validation accuracy is:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As anticipated, the performance of the model trained on the bottleneck features is much better then the performance of the model trained on raw images. Our approach works. The next step is to find the most optimal classifier. We will accomplish that using the AML feature called `Automated ML`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated model selection\n",
    "\n",
    "Automated machine learning (automated ML) automatically picks an algorithm and hyperparameters that optimize a given  primary metric. The model can be downloaded to be further customized as well. There are several options that you can use to configure Automated ML experiments. In this section of the lab we will go through the steps to configure and run the `Automated ML` job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment\n",
    "\n",
    "We will create a new experiment to manage `Automated ML' runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'aerial-automatedML'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create  compute target\n",
    "\n",
    "Automated ML supports running concurrent experiments on Azure Batch AI clusters. This can significantly shorten the model selection and hyperparameter optimization process. \n",
    "\n",
    "**Note:** The creation of the Batch AI cluster can take over 10 minutes, please be patient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import BatchAiCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "batchai_cluster_name = \"batchaicls\"\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "for ct_name, ct in ws.compute_targets.items():\n",
    "    print(ct.name, ct.type)\n",
    "    if (ct.name == batchai_cluster_name and ct.type == 'BatchAI'):\n",
    "        found = True\n",
    "        print('Found existing compute target.')\n",
    "        bai_compute_target = ct\n",
    "        break\n",
    "        \n",
    "if not found:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = BatchAiCompute.provisioning_configuration(vm_size = \"STANDARD_DS2_V2\", \n",
    "                                                                autoscale_enabled = True,\n",
    "                                                                cluster_min_nodes = 1, \n",
    "                                                                cluster_max_nodes = 5)\n",
    "\n",
    "    # Create the cluster.\n",
    "    bai_compute_target = ComputeTarget.create(ws, batchai_cluster_name, provisioning_config)\n",
    "    \n",
    "    # Can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "    bai_compute_target.wait_for_completion(show_output = True, min_node_count = None, timeout_in_minutes = 20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Get Data script\n",
    "\n",
    "If you are using a remote compute to run your Automated ML experiments, the data fetch must be wrapped in a separate python script that implements `get_data()` function. This script is run on the remote compute where the automated ML experiment is run. `get_data()` eliminates the need to fetch the data over the wire for each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './script'\n",
    "script_name = 'get_data.py'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/get_data.py\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data():\n",
    "    # Load bottleneck features\n",
    "    data_folder = os.environ[\"AZUREML_DATAREFERENCE_workspacefilestore\"]\n",
    "    file_name = os.path.join(data_folder, 'aerial_bottleneck_resnet50.h5')\n",
    "    \n",
    "    print(\"Data folder:\", data_folder)\n",
    "    print(\"Bottleneck features file:\", file_name)\n",
    "    print(\"Data folder content:\", os.listdir(data_folder))\n",
    "    \n",
    "    with h5py.File(file_name, \"r\") as hfile:\n",
    "        features = np.array(hfile.get('features'))\n",
    "        labels = np.array(hfile.get('labels'))\n",
    "        \n",
    "    # Split the data into training and validation partitions   \n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(features, labels,\n",
    "                                                               test_size=0.1,\n",
    "                                                               shuffle=True,\n",
    "                                                               stratify=labels)\n",
    "        \n",
    "\n",
    "    return {'X': X_train, 'y': y_train, 'X_valid': X_validation, 'y_valid': y_validation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure datastore and data reference\n",
    "\n",
    "The bottleneck files have been uploaded to the workspace's default datastore during the previous step. We will download the files onto the nodes of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "from azureml.core.runconfig import DataReferenceConfiguration\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "print(\"Using the default datastore for training data: \")\n",
    "print(ds.name, ds.datastore_type, ds.account_name, ds.container_name)\n",
    "\n",
    "dr = DataReferenceConfiguration(datastore_name=ds.name, \n",
    "                   path_on_datastore='bottleneck_features', \n",
    "                   path_on_compute='bottleneck_features',\n",
    "                   mode='download', # download files from datastore to compute target\n",
    "                   overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Docker run configuration\n",
    "\n",
    "We will run `Automated ML` jobs in a custom docker image that will include dependencies required by `get_data()` script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core import Run\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "# create a new RunConfig object\n",
    "run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Azure Batch AI cluster for Automated ML jobs require docker.\n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# Set compute target to BAI cluster\n",
    "run_config.target = bai_compute_target.name\n",
    "\n",
    "# Set data references\n",
    "run_config.data_references = {ds.name: dr}\n",
    "\n",
    "# specify packages required by get_data\n",
    "run_config.environment.python.conda_dependencies = \\\n",
    "   CondaDependencies.create(conda_packages=['h5py'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Automated ML run.\n",
    "\n",
    "Automated ML runs can be controlled using a number of configuration parameters. \n",
    "\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|classification or regression|\n",
    "|**primary_metric**|This is the metric that you want to optimize.<br> Classification supports the following primary metrics <br><i>accuracy</i><br><i>AUC_weighted</i><br><i>balanced_accuracy</i><br><i>average_precision_score_weighted</i><br><i>precision_score_weighted</i>|\n",
    "|**max_time_sec**|Time limit in seconds for each iteration|\n",
    "|**iterations**|Number of iterations. In each iteration Auto ML trains a specific pipeline with the data|\n",
    "|**n_cross_validations**|Number of cross validation splits|\n",
    "|**concurrent_iterations**|Max number of iterations that would be executed in parallel. |\n",
    "|**preprocess**| *True/False* <br>Setting this to *True* enables Auto ML to perform preprocessing <br>on the input to handle *missing data*, and perform some common *feature extraction*|\n",
    "|**max_cores_per_iteration**| Indicates how many cores on the compute target would be used to train a single pipeline.<br> Default is *1*, you can set it to *-1* to use all cores|\n",
    "|**exit_score**|*double* value indicating the target for *primary_metric*. <br>Once the target is surpassed the run terminates.|\n",
    "|**blacklist_algos**|*List* of *strings* indicating machine learning algorithms for AutoML to avoid in this run.<br><br> Allowed values for **Classification**<br><i>LogisticRegression</i><br><i>SGDClassifierWrapper</i><br><i>NBWrapper</i><br><i>BernoulliNB</i><br><i>SVCWrapper</i><br><i>LinearSVMWrapper</i><br><i>KNeighborsClassifier</i><br><i>GradientBoostingClassifier</i><br><i>DecisionTreeClassifier</i><br><i>RandomForestClassifier</i><br><i>ExtraTreesClassifier</i><br><i>LightGBMClassifier</i><br><br>Allowed values for **Regression**<br><i>ElasticNet<i><br><i>GradientBoostingRegressor<i><br><i>DecisionTreeRegressor<i><br><i>KNeighborsRegressor<i><br><i>LassoLars<i><br><i>SGDRegressor<i><br><i>RandomForestRegressor<i><br><i>ExtraTreesRegressor<i>|\n",
    "|**path**|Relative path to the project folder. AutoML stores configuration files for the experiment under this folder. You can specify a new empty folder.|\n",
    "    \n",
    "For the optimal performance of `AutomatedML` it is recommended to run at least 100 iterations. Due to the lab's time constraints we will only run 25 iterations. We will also limit a number of alogirthms tried using the `blacklist_algos` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "import logging\n",
    "\n",
    "\n",
    "automl_config = AutoMLConfig(run_configuration = run_config,\n",
    "                             task = 'classification',\n",
    "                             num_classes = 6,\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             primary_metric = 'accuracy',\n",
    "                             max_time_sec = 1800,\n",
    "                             iterations = 25,\n",
    "                             concurrent_iterations = 5,\n",
    "                             max_cores_per_iteration = 1,\n",
    "                             preprocess = False,\n",
    "                             exit_score = 0.96,\n",
    "                             blacklist_algos = ['KNeighborsClassifier',\n",
    "                                                'LinearSVMWrapper',\n",
    "                                                'NBWrapper',\n",
    "                                                'BernoulliNB',\n",
    "                                                'GradientBoostingClassifier',\n",
    "                                                'SGDClassifierWrapper'],\n",
    "                             verbosity = logging.INFO,\n",
    "                             path = script_folder,\n",
    "                             data_script = os.path.join(script_folder, script_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Automated ML job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {\"Desc\": \"automated ml\"}\n",
    "run = exp.submit(config=automl_config, tags=tags)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call to experiment returns `AutoMLRun` object that can be used to track the run.\n",
    "\n",
    "Since the call is asynchronous, it reports a **Preparing** or **Running** state as soon as the job is started.\n",
    "\n",
    "Here is what's happening while you wait:\n",
    "\n",
    "- **Image creation**: A Docker image is created matching the Python environment specified by the RunConfiguration. The image is uploaded to the workspace. This happens only once for each Python environment since the container is cached for subsequent runs.  During image creation, logs are streamed to the run history. You can monitor the image creation progress using these logs.\n",
    "\n",
    "- **Scaling**: If the remote cluster requires more nodes to execute the run than currently available, additional nodes are added automatically. \n",
    "\n",
    "- **Running**: In this stage, the Automated ML takes over and starts running experiments\n",
    "\n",
    "\n",
    "\n",
    "You can check the progress of a running job in multiple ways: Azure Portal, AML Widgets or streaming logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor the run.\n",
    "\n",
    "We will use AML Widget to monitor the run. The widget will first report a \"loading\" status while running the first iteration. After completing the first iteration, an auto-updating graph and table will be shown. The widget will refresh once per minute, so you should see the graph update as child runs complete.\n",
    "\n",
    "The widget is asynchronous - it does not block the notebook. You can execute other cells while the widget is running.\n",
    "\n",
    "**Note:** The widget displays a link at the bottom. Use this link to open a web interface to explore the individual run details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cancelling Runs\n",
    "\n",
    "You can cancel ongoing remote runs using the `cancel` and `cancel_iteration` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel the ongoing experiment and stop scheduling new iterations.\n",
    "# run.cancel()\n",
    "\n",
    "# Cancel iteration 1 and move onto iteration 2.\n",
    "# run.cancel_iteration(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the run\n",
    "\n",
    "You can  use SDK methods to fetch all the child runs and see individual metrics that we log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "children = list(run.get_children())\n",
    "metricslist = {}\n",
    "for child in children:\n",
    "    properties = child.get_properties()\n",
    "    metrics = {k: v for k, v in child.get_metrics().items() if isinstance(v, float)}\n",
    "    metricslist[int(properties['iteration'])] = metrics\n",
    "\n",
    "rundata = pd.DataFrame(metricslist).sort_index(1)\n",
    "rundata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waiting until the run finishes\n",
    "\n",
    "`wait_for_complettion` method will block till the run finishes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the run finishes.\n",
    "run.wait_for_completion(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the best model\n",
    "\n",
    "Below we select the best pipeline from our iterations. The `get_output` method returns the best run and the fitted model. The Model includes the pipeline and any pre-processing.  Overloads on `get_output` allow you to retrieve the best run and fitted model for *any* logged metric or for a particular *iteration*.\n",
    "\n",
    "#### Best model based on the primary metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = run.get_output()\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best model based on any metric\n",
    "You can also retrieve the best model based on an arbitrary metric. For example, we will retrieve the model with the highest AUC value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_metric = \"AUC_weighted\"\n",
    "best_run_auc, fitted_model_best_auc = run.get_output(metric = lookup_metric)\n",
    "print(best_run_auc)\n",
    "print(fitted_model_best_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model from a specific iteration\n",
    "Show the run and the model from the third iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 3\n",
    "third_run, third_model = run.get_output(iteration=iteration)\n",
    "print(third_run)\n",
    "print(third_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the best model\n",
    "\n",
    "You can use `AutoMLRun` to register the trained model with AML Model Registry.\n",
    "\n",
    "If neither `metric` nor `iteration` are specified in the `register_model` call, the iteration with the best primary metric is registered.\n",
    "\n",
    "We are going to add two properties to the registration:\n",
    "- FriendlyName. We will use this property to retrieve the model in the next lab\n",
    "- RunID. This property will store the ID of the run that created the model. We will use it to retrieve runtime dependencies in the next lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = 'Aerial Classifier Model'\n",
    "\n",
    "model = run.register_model(description=description, tags=tags)\n",
    "properties = {\"FriendlyName\": \"Lab3BestModel\", \"RunID\": run.id}\n",
    "model.add_properties(properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "The model is ready for deployment. In the next lab you will deploy the model to Azure Container Instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "\n",
    "Before you move to the next step, you can delete the BAI cluster. We will not need it anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bai_compute_target.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
