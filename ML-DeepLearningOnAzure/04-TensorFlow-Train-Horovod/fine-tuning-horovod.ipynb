{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Custom Image Classification Model\n",
    "\n",
    "In this lab, you will developed a custom image classification model to automatically classify the type of land shown in aerial images of 224-meter x 224-meter plots. Land use classification models can be used to track urbanization, deforestation, loss of wetlands, and other major environmental trends using periodically collected aerial imagery. The images used in this lab are based off of imagery from the U.S. National Land Cover Database. U.S. National Land Cover Database defines six primary classes of land use: *Developed*, *Barren*, *Forested*, *Grassland*, *Shrub*, *Cultivated*. Example images from each land use class are shown here:\n",
    "\n",
    "Developed | Cultivated | Barren\n",
    "--------- | ------ | ----------\n",
    "![Developed](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/developed1.png) | ![Cultivated](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/cultivated1.png) | ![Barren](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/barren1.png)\n",
    "\n",
    "Forested | Grassland | Shrub\n",
    "---------| ----------| -----\n",
    "![Forested](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/forest1.png) | ![Grassland](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/grassland1.png) | ![Shrub](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/shrub1.png)\n",
    "\n",
    "You will develop a custom image classifier using a Deep Learning technique called *Fine-tuning*. *Fine-tuning* is a flavor of Transfer learning, which is one of the fastest (code and run-time-wise) ways to start using deep learning. It allows for the reuse of knowledge gained while solving one problem to a different but related problem. For example, knowledge gained while learning to recognize landmarks and landscapes could apply when trying to recognize aerial land plots. Transfer Learning makes it feasible to train very effective ML models on relatively small training data sets.\n",
    "\n",
    "In fine-tuning, you remove the last layer(s) (usually the FCNN layers) of the pre-trained network and replace it with the new untrained layers that match the given ML task. You than re-train the network using images from your custom domain. It is also a common practice to freeze the weights of the first few layers of the pre-trained network. This is because these layers capture universal features like curves and edges that are also relevant to the new problem. You want to keep those weights intact. Instead, you \"force\" the network to focus on learning dataset-specific features in the subsequent layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to AML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /data/home/demouser/notebooks/aml_config/config.json\n",
      "jkamlworkshop\n",
      "jkamlworkshop\n",
      "southcentralus\n",
      "952a710c-8d9c-40c1-9fec-f752138cc0b3\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure ML Managed Compute\n",
    "\n",
    "To run the lab's scripts we will utilize Azure ML managed compute resources. Specifically, an autoscale cluster of *Standard_NC6* VMs (equipped with Tesla K80 GPU). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating a new compute target...\n",
      "Creating\n",
      "Succeeded.........\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n",
      "{'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-01-13T21:28:25.469000+00:00', 'creationTime': '2019-01-13T21:27:11.053008+00:00', 'currentNodeCount': 1, 'errors': None, 'modifiedTime': '2019-01-13T21:27:39.253893+00:00', 'nodeStateCounts': {'idleNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0, 'preparingNodeCount': 1, 'runningNodeCount': 0, 'unusableNodeCount': 0}, 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 1, 'maxNodeCount': 2, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'targetNodeCount': 1, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"gpu-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 1)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 2)\n",
    "\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"Standard_NC6\")\n",
    "#vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"Standard_NC6s_v3\")\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "     # For a more detailed view of current AmlCompute status, use the 'status' property    \n",
    "    print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train new top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "batchsize=64\n",
    "\n",
    "train_data_dir = '/tmp/datasets/aerialsmall/train'\n",
    "valid_data_dir = '/tmp/datasets/aerialsmall/valid'\n",
    "classes = [\"Barren\", \"Cultivated\", \"Developed\", \"Forest\", \"Herbaceous\", \"Shrub\"]   \n",
    "\n",
    "# A hack to mitigate a bug in Keras\n",
    "def preprocess_input_new(x):\n",
    "    img = vgg16.preprocess_input(image.img_to_array(x))\n",
    "    return image.array_to_img(img)\n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_new)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=train_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    classes=classes,\n",
    "    batch_size=batchsize)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_new)\n",
    "valid_generator = train_datagen.flow_from_directory(\n",
    "    directory=valid_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    classes=classes,\n",
    "    batch_size=batchsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Use adadelta optimizer for pretraining the top layer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = 'adadelta',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=20,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=len(valid_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make last convolutional layer trainable\n",
    "for layer in base_model.layers[:14]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "for layer in base_model.layers[14:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# For fine tuning use SGD with a low learning rate\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=50,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=len(valid_generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Pre-train the new top layers\n",
    "\n",
    "#### Create training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./script/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, Input\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.applications import vgg16\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "\n",
    "# Create custom callback to track accuracy measures in AML Experiment\n",
    "class RunCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, run):\n",
    "        self.run = run\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.run.log(name=\"training_acc\", value=float(logs.get('acc')))\n",
    "        self.run.log(name=\"validation_acc\", value=float(logs.get('val_acc')))\n",
    "\n",
    "\n",
    "def custom_classifier(input_shape=(224,224,3), units=256, classes=6,  l1=0.01, l2=0.01, optimizer='adadelta'):\n",
    "    # Create a base vgg16 model\n",
    "    base_model = vgg16.VGG16(\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        pooling='avg')\n",
    "    # Add new top\n",
    "    x = base_model.output\n",
    "    x = Dense(units, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    y = Dense(classes, activation='softmax', kernel_regularizer=l1_l2(l1=l1, l2=l2))(x)\n",
    "    model = Model(inputs=base_model.inputs, outputs=y)\n",
    "    \n",
    "    return model, base_model\n",
    "    \n",
    "        \n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# Default global parameters\n",
    "tf.app.flags.DEFINE_integer('batch_size', 32, \"Number of images per batch\")\n",
    "tf.app.flags.DEFINE_integer('epochs', 10, \"Number of epochs to train\")\n",
    "tf.app.flags.DEFINE_integer('units', 512, \"Number of epochs to train\")\n",
    "tf.app.flags.DEFINE_string('data_folder', 'aerialsmall', \"Folder with images\")\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    \n",
    "    \n",
    "    print(\"Loading data from:\", FLAGS.data_folder)\n",
    "    # Create training and validation data generators\n",
    "    train_data_dir = os.path.join(FLAGS.data_folder, 'train')\n",
    "    valid_data_dir = os.path.join(FLAGS.data_folder, 'valid')\n",
    "     \n",
    "    # A hack to mitigate a bug in TF.Keras 1.12\n",
    "    def preprocess_input_new(x):\n",
    "        img = vgg16.preprocess_input(image.img_to_array(x))\n",
    "        return image.array_to_img(img)\n",
    "    \n",
    "    batchsize=64\n",
    "    classes = [\"Barren\", \"Cultivated\", \"Developed\", \"Forest\", \"Herbaceous\", \"Shrub\"]\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_new)\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        directory=train_data_dir,\n",
    "        target_size=(224, 224),\n",
    "        classes=classes,\n",
    "        batch_size=batchsize)\n",
    "\n",
    "    valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_new)\n",
    "    valid_generator = train_datagen.flow_from_directory(\n",
    "        directory=valid_data_dir,\n",
    "        target_size=(224, 224),\n",
    "        classes=classes,\n",
    "        batch_size=batchsize)\n",
    "    \n",
    "    print(len(train_generator))\n",
    "    print(len(valid_generator))\n",
    "    \n",
    "    \n",
    "    # Create a custom model\n",
    "    model, base_model = custom_classifier()\n",
    "    \n",
    "    # freeze all base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Use adadelta optimizer for pretraining the top layer\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = 'adadelta',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    # Configure callbacks to generate Tensorboard and AML logs\n",
    "    \n",
    "    # get hold of the current run\n",
    "    run = Run.get_submitted_run()\n",
    "    callbacks = [tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "                 RunCallback(run)]\n",
    "    \n",
    "    # Start training\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=FLAGS.epochs,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=valid_generator,\n",
    "        validation_steps=len(valid_generator))\n",
    "    \n",
    "    # Save the trained model to outputs which is a standard folder expected by AML\n",
    "    print(\"Training completed.\")\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    model_file = os.path.join('outputs', 'aerial_model_pretrain.h5')\n",
    "    weights_file = os.path.join('outputs', 'aerial_model_weights_pretrain.h5')\n",
    "    print(\"Saving model to: {0}\".format(model_file))\n",
    "    model.save(model_file)\n",
    "    print(\"Saving model weights to: {0}\".format(weights_file))\n",
    "    model.save_weights(weights_file)\n",
    " \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure datastore\n",
    "\n",
    "The training images have been uploaded to a public Azure blob storage container. We will register this container as an AML Datastore within our workspace. Before the data prep script runs, the datastore's content - training images - will be copied to the local storage on compute nodes.\n",
    "\n",
    "After the script completes, its output - the bottleneck features file - will be uploaded by AML to the workspace's default datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new datastore for input images\n",
      "training_images AzureBlob azureailabs aerialsmall\n",
      "Using the default datastore for output: \n",
      "workspaceblobstore AzureBlob jkamlworstoragedirzndtd azureml-blobstore-5635fa3a-71d5-4d1b-bc80-b0847d6f842b\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "images_account = 'azureailabs'\n",
    "images_container = 'aerialsmall'\n",
    "datastore_name = 'training_images'\n",
    "\n",
    "# Check if the datastore exists. If not create a new one\n",
    "try:\n",
    "    input_ds = Datastore.get(ws, datastore_name)\n",
    "    print('Found existing datastore for input images:', input_ds.name)\n",
    "except:\n",
    "    input_ds = Datastore.register_azure_blob_container(workspace=ws, datastore_name=datastore_name,\n",
    "                                            container_name=images_container,\n",
    "                                            account_name=images_account)\n",
    "    print('Creating new datastore for input images')\n",
    "\n",
    " \n",
    "   \n",
    "print(input_ds.name, input_ds.datastore_type, input_ds.account_name, input_ds.container_name)\n",
    "\n",
    "output_ds = ws.get_default_datastore()\n",
    "print(\"Using the default datastore for output: \")\n",
    "print(output_ds.name, output_ds.datastore_type, output_ds.account_name, output_ds.container_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AML Experiment\n",
    "We will track runs of the feature extraction script in a dedicated Experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'aerial-finetune-pretrain'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a pre-training on a single node of the cluster\n",
    "\n",
    "Due to time limitations of the lab, we will run pre-training for 2 epochs only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data_folder': input_ds.as_download(),\n",
    "    '--epochs': 2\n",
    "}\n",
    "\n",
    "pip_packages = ['h5py', \n",
    "                'pillow', \n",
    "                'scikit-learn', \n",
    "                'tqdm', \n",
    "                'tensorflow-gpu==1.12']\n",
    "\n",
    "\n",
    "est = Estimator(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='train.py',\n",
    "                node_count=1,\n",
    "                process_count_per_node=1,\n",
    "                use_gpu=True,\n",
    "                pip_packages=pip_packages,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>aerial-finetune-pretrain</td><td>aerial-finetune-pretrain_1547419586666</td><td>azureml.scriptrun</td><td>Queued</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/952a710c-8d9c-40c1-9fec-f752138cc0b3/resourceGroups/jkamlworkshop/providers/Microsoft.MachineLearningServices/workspaces/jkamlworkshop/experiments/aerial-finetune-pretrain/runs/aerial-finetune-pretrain_1547419586666\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: aerial-finetune-pretrain,\n",
       "Id: aerial-finetune-pretrain_1547419586666,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Queued)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = {\"Run Type\": \"Top pre-train\"}\n",
    "run = exp.submit(est, tags=tags)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://svcusscds1:6006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://svcusscds1:6006'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.contrib.tensorboard import Tensorboard\n",
    "tb = Tensorboard([run])\n",
    "tb.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can cancel the run with the `cancel` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azureml-logs/60_control_log.txt', 'azureml-logs/80_driver_log.txt', 'logs/events.out.tfevents.1547419788.8903cdfb81314734996b299332e698e8000000', 'outputs/aerial_model_pretrain.h5', 'outputs/aerial_model_weights_pretrain.h5', 'driver_log', 'azureml-logs/azureml.log']\n"
     ]
    }
   ],
   "source": [
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file('outputs//aerial_model_weights_pretrain.h5', '/tmp/models/aerial_model_weights_pretrain.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aerial_model_weights_pretrain.h5\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "ls /tmp/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload weights to the default datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AzureBlob jkamlworstoragedirzndtd azureml-blobstore-5635fa3a-71d5-4d1b-bc80-b0847d6f842b\n",
      "Uploading /tmp/models/aerial_model_weights_pretrain.h5\n",
      "Uploaded /tmp/models/aerial_model_weights_pretrain.h5, 1 files out of an estimated total of 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_9bcbdacbd10a460faad8c691e870deb3"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the dataset to the DataStore\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "print(ds.datastore_type, ds.account_name, ds.container_name)\n",
    "ds.upload(src_dir='/tmp/models', target_path='models', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure fine-tuning job\n",
    "\n",
    "#### Create training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./script/fine-tune.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/fine-tune.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, Input\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.applications import vgg16\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "\n",
    "def custom_classifier(input_shape=(224,224,3), units=256, classes=6,  l1=0.01, l2=0.01, optimizer='adadelta'):\n",
    "    # Create a base vgg16 model\n",
    "    base_model = vgg16.VGG16(\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        pooling='avg')\n",
    "    # Add new top\n",
    "    x = base_model.output\n",
    "    x = Dense(units, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    y = Dense(classes, activation='softmax', kernel_regularizer=l1_l2(l1=l1, l2=l2))(x)\n",
    "    model = Model(inputs=base_model.inputs, outputs=y)\n",
    "    \n",
    "    return model, base_model\n",
    "    \n",
    "        \n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# Default global parameters\n",
    "tf.app.flags.DEFINE_integer('batch_size', 32, \"Number of images per batch\")\n",
    "tf.app.flags.DEFINE_integer('epochs', 10, \"Number of epochs to train\")\n",
    "tf.app.flags.DEFINE_integer('units', 512, \"Number of epochs to train\")\n",
    "tf.app.flags.DEFINE_string('data_folder', 'aerialsmall', \"Folder with images\")\n",
    "tf.app.flags.DEFINE_string('weights_folder', 'models', \"Folder with model weights\")\n",
    "tf.app.flags.DEFINE_string('weights_filename', 'aerial_model_weights_pretrain.h5', \"Folder with model weights\")\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    \n",
    "    # get hold of the current run\n",
    "    run = Run.get_submitted_run()\n",
    "    \n",
    "    print(\"Loading data from:\", FLAGS.data_folder)\n",
    "    # Create training and validation data generators\n",
    "    train_data_dir = os.path.join(FLAGS.data_folder, 'train')\n",
    "    valid_data_dir = os.path.join(FLAGS.data_folder, 'valid')\n",
    "    \n",
    "    # A hack to mitigate a bug in TF.Keras 1.12\n",
    "    def preprocess_input_new(x):\n",
    "        img = vgg16.preprocess_input(image.img_to_array(x))\n",
    "        return image.array_to_img(img)\n",
    "    \n",
    "    batchsize=64\n",
    "    classes = [\"Barren\", \"Cultivated\", \"Developed\", \"Forest\", \"Herbaceous\", \"Shrub\"]\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_new)\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        directory=train_data_dir,\n",
    "        target_size=(224, 224),\n",
    "        classes=classes,\n",
    "        batch_size=batchsize)\n",
    "\n",
    "    valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_new)\n",
    "    valid_generator = train_datagen.flow_from_directory(\n",
    "        directory=valid_data_dir,\n",
    "        target_size=(224, 224),\n",
    "        classes=classes,\n",
    "        batch_size=batchsize)\n",
    "    \n",
    "    print(len(train_generator))\n",
    "    print(len(valid_generator))\n",
    "    \n",
    "    \n",
    "    # Create a custom model\n",
    "    model, base_model = custom_classifier()\n",
    "    \n",
    "    weights_file = os.path.join(FLAGS.weights_folder, FLAGS.weights_filename)\n",
    "    model.load_weights(weights_file)\n",
    "    \n",
    "    # Make last convolutional layer trainable\n",
    "    for layer in base_model.layers[:14]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    for layer in base_model.layers[14:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # For fine tuning use SGD with a low learning rate\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=10,\n",
    "        validation_data=valid_generator,\n",
    "        validation_steps=len(valid_generator))\n",
    "    \n",
    "    # Save the trained model to outputs which is a standard folder expected by AML\n",
    "    print(\"Training completed.\")\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    model_file = os.path.join('outputs', 'aerial_model_pretrain.hd5')\n",
    "    weights_file = os.path.join('outputs', 'aerial_model_weights_pretrain.hd5')\n",
    "    print(\"Saving model to: {0}\".format(model_file))\n",
    "    model.save(model_file, save_format='h5')\n",
    "    print(\"Saving model weights to: {0}\".format(weights_file))\n",
    "    model.save_weights(weights_file, save_format='h5')\n",
    " \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AML Experiment\n",
    "We will track runs of the feature extraction script in a dedicated Experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'aerial-finetune-train'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a fine-tuning training on  the cluster\n",
    "\n",
    "Due to time limitations of the lab, we will run pre-training for 2 epochs only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data_folder': input_ds.as_download(),\n",
    "    '--weights_folder': ds.path('models').as_download(),\n",
    "    '--epochs': 2\n",
    "}\n",
    "\n",
    "pip_packages = ['h5py', \n",
    "                'pillow', \n",
    "                'scikit-learn', \n",
    "                'tqdm', \n",
    "                'tensorflow-gpu==1.12']\n",
    "\n",
    "\n",
    "est = Estimator(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='fine-tune.py',\n",
    "                node_count=1,\n",
    "                process_count_per_node=1,\n",
    "                use_gpu=True,\n",
    "                pip_packages=pip_packages,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>aerial-finetune-pretrain</td><td>aerial-finetune-pretrain_1547423391085</td><td>azureml.scriptrun</td><td>Queued</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/952a710c-8d9c-40c1-9fec-f752138cc0b3/resourceGroups/jkamlworkshop/providers/Microsoft.MachineLearningServices/workspaces/jkamlworkshop/experiments/aerial-finetune-pretrain/runs/aerial-finetune-pretrain_1547423391085\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: aerial-finetune-pretrain,\n",
       "Id: aerial-finetune-pretrain_1547423391085,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Queued)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = {\"Run Type\": \"Top pre-train\"}\n",
    "run = exp.submit(est, tags=tags)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4309f8006443f68f5ef80a34eeb380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "ls /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7212294"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '/tmp/datasets/aerialsmall/train'\n",
    "valid_data_dir = '/tmp/datasets/aerialsmall/valid'\n",
    "\n",
    "train_generator = ImageGenerator(train_data_dir, preprocess_fn=vgg16.preprocess_input)\n",
    "valid_generator = ImageGenerator(valid_data_dir, preprocess_fn=vgg16.preprocess_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
