{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Custom Image Classification Model\n",
    "\n",
    "In this lab, you will developed a custom image classification model to automatically classify the type of land shown in aerial images of 224-meter x 224-meter plots. Land use classification models can be used to track urbanization, deforestation, loss of wetlands, and other major environmental trends using periodically collected aerial imagery. The images used in this lab are based off of imagery from the U.S. National Land Cover Database. U.S. National Land Cover Database defines six primary classes of land use: *Developed*, *Barren*, *Forested*, *Grassland*, *Shrub*, *Cultivated*. Example images from each land use class are shown here:\n",
    "\n",
    "Developed | Cultivated | Barren\n",
    "--------- | ------ | ----------\n",
    "![Developed](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/developed1.png) | ![Cultivated](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/cultivated1.png) | ![Barren](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/barren1.png)\n",
    "\n",
    "Forested | Grassland | Shrub\n",
    "---------| ----------| -----\n",
    "![Forested](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/forest1.png) | ![Grassland](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/grassland1.png) | ![Shrub](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/shrub1.png)\n",
    "\n",
    "You will develop a custom image classifier using a Deep Learning technique called *Fine-tuning*. *Fine-tuning* is a flavor of Transfer learning, which is one of the fastest (code and run-time-wise) ways to start using deep learning. It allows for the reuse of knowledge gained while solving one problem to a different but related problem. For example, knowledge gained while learning to recognize landmarks and landscapes could apply when trying to recognize aerial land plots. Transfer Learning makes it feasible to train very effective ML models on relatively small training data sets.\n",
    "\n",
    "In fine-tuning, you remove the last layer(s) (usually the FCNN layers) of the pre-trained network and replace it with the new untrained layers that match the given ML task. You than re-train the network using images from your custom domain. It is also a common practice to freeze the weights of the first few layers of the pre-trained network. This is because these layers capture universal features like curves and edges that are also relevant to the new problem. You want to keep those weights intact. Instead, you \"force\" the network to focus on learning dataset-specific features in the subsequent layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the lab dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "azcopy --source https://azureailabs.blob.core.windows.net/aerialsmall --destination /tmp/datasets/aerialsmall --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "ls -l /tmp/datasets/aerialsmall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to AML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /data/home/demouser/notebooks/aml_config/config.json\n",
      "jkamlworkshop\n",
      "jkamlworkshop\n",
      "southcentralus\n",
      "952a710c-8d9c-40c1-9fec-f752138cc0b3\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the dataset to the default Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "print(ds.datastore_type, ds.account_name, ds.container_name)\n",
    "\n",
    "ds.upload(src_dir='/tmp/datasets/aerialsmall', target_path='aerialsmall', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure ML Managed Compute\n",
    "\n",
    "To run the lab's scripts we will utilize Azure ML managed compute resources. Specifically, an autoscale cluster of *Standard_NC6* VMs (equipped with Tesla K80 GPU). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target. just use it. gpu-cluster\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"gpu-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 1)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 3)\n",
    "\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"Standard_NC6\")\n",
    "#vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"Standard_NC6s_v3\")\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "     # For a more detailed view of current AmlCompute status, use the 'status' property    \n",
    "    print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Pre-training\n",
    "\n",
    "#### Create training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./script/pre-train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/pre-train.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, Input\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.applications import vgg16\n",
    "\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "\n",
    "# Create custom callback to track accuracy measures in AML Experiment\n",
    "class RunCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, run):\n",
    "        self.run = run\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.run.log(name=\"training_acc\", value=float(logs.get('acc')))\n",
    "        self.run.log(name=\"validation_acc\", value=float(logs.get('val_acc')))\n",
    "\n",
    "\n",
    "def custom_classifier(input_shape=(224,224,3), units=256, classes=6,  l1=0.01, l2=0.01, optimizer='adadelta'):\n",
    "    # Create a base vgg16 model\n",
    "    base_model = vgg16.VGG16(\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        pooling='avg')\n",
    "    # Add new top\n",
    "    x = base_model.output\n",
    "    x = Dense(units, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    y = Dense(classes, activation='softmax', kernel_regularizer=l1_l2(l1=l1, l2=l2))(x)\n",
    "    model = Model(inputs=base_model.inputs, outputs=y)\n",
    "    \n",
    "    return model, base_model\n",
    "       \n",
    "\n",
    "def main(argv=None):\n",
    "    \n",
    "    \n",
    "    print(\"Loading data from:\", FLAGS.data_folder)\n",
    "    # Create training and validation data generators\n",
    "    train_data_dir = os.path.join(FLAGS.data_folder, 'train')\n",
    "    valid_data_dir = os.path.join(FLAGS.data_folder, 'valid')\n",
    "     \n",
    "    # A hack to mitigate a bug in TF.Keras 1.12\n",
    "    def preprocess_input_new(x):\n",
    "        img = vgg16.preprocess_input(image.img_to_array(x))\n",
    "        return image.array_to_img(img)\n",
    "    \n",
    "    batchsize=64\n",
    "    classes = [\"Barren\", \"Cultivated\", \"Developed\", \"Forest\", \"Herbaceous\", \"Shrub\"]\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_new)\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        directory=train_data_dir,\n",
    "        target_size=(224, 224),\n",
    "        classes=classes,\n",
    "        batch_size=batchsize)\n",
    "\n",
    "    valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_new)\n",
    "    valid_generator = train_datagen.flow_from_directory(\n",
    "        directory=valid_data_dir,\n",
    "        target_size=(224, 224),\n",
    "        classes=classes,\n",
    "        batch_size=batchsize)\n",
    "    \n",
    "    print(len(train_generator))\n",
    "    print(len(valid_generator))\n",
    "    \n",
    "    \n",
    "    # Create a custom model\n",
    "    model, base_model = custom_classifier()\n",
    "    \n",
    "    # freeze all base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Use adadelta optimizer for pretraining the top layer\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = 'adadelta',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    # Configure callbacks to generate Tensorboard and AML logs\n",
    "    run = Run.get_submitted_run()\n",
    "    callbacks = [tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "                 RunCallback(run)]\n",
    "    \n",
    "    # Start training\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=FLAGS.epochs,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=valid_generator,\n",
    "        validation_steps=len(valid_generator))\n",
    "    \n",
    "    # Save the trained model to outputs which is a standard folder expected by AML\n",
    "    print(\"Training completed.\")\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    model_file = os.path.join('outputs', 'aerial_model_pretrain.h5')\n",
    "    weights_file = os.path.join('outputs', 'aerial_model_weights_pretrain.h5')\n",
    "    print(\"Saving model to: {0}\".format(model_file))\n",
    "    model.save(model_file)\n",
    "    print(\"Saving model weights to: {0}\".format(weights_file))\n",
    "    model.save_weights(weights_file)\n",
    " \n",
    "\n",
    "# Default global parameters\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('batch_size', 32, \"Number of images per batch\")\n",
    "tf.app.flags.DEFINE_integer('epochs', 10, \"Number of epochs to train\")\n",
    "tf.app.flags.DEFINE_integer('units', 512, \"Number of epochs to train\")\n",
    "tf.app.flags.DEFINE_string('data_folder', 'aerialsmall', \"Folder with images\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create AML Experiment\n",
    "We will track pre-traning in a dedicated Experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'aerial-finetune-pretrain'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a pre-training on a single node of the cluster\n",
    "\n",
    "Due to time limitations of the lab, we will run pre-training for 3 epochs only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "script_params = {\n",
    "    '--data_folder': ds.path('aerialsmall').as_mount(),\n",
    "    '--epochs': 3\n",
    "}\n",
    "\n",
    "pip_packages = ['h5py', 'pillow', 'scipy', 'tensorflow-gpu==1.12']\n",
    "\n",
    "est = TensorFlow(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='pre-train.py',\n",
    "                use_gpu=True,\n",
    "                pip_packages=pip_packages\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>aerial-finetune-pretrain</td><td>aerial-finetune-pretrain_1547433240121</td><td>azureml.scriptrun</td><td>Starting</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/952a710c-8d9c-40c1-9fec-f752138cc0b3/resourceGroups/jkamlworkshop/providers/Microsoft.MachineLearningServices/workspaces/jkamlworkshop/experiments/aerial-finetune-pretrain/runs/aerial-finetune-pretrain_1547433240121\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: aerial-finetune-pretrain,\n",
       "Id: aerial-finetune-pretrain_1547433240121,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Starting)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = {\"Run Type\": \"Top pre-train\"}\n",
    "run = exp.submit(est, tags=tags)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e0e552e07b4e32aafb21a6888b3f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://svcusscds1:6006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://svcusscds1:6006'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.contrib.tensorboard import Tensorboard\n",
    "tb = Tensorboard([run])\n",
    "tb.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can cancel the run with the `cancel` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExperimentExecutionException",
     "evalue": "{\n    \"error_details\": {\n        \"correlation\": {\n            \"operation\": \"42cc789eb553c941ba9a44d47437c419\",\n            \"request\": \"5LAT7DLlykQ=\"\n        },\n        \"error\": {\n            \"code\": \"UserError\",\n            \"debugInfo\": {\n                \"innerException\": {\n                    \"errorResponse\": {\n                        \"error\": {\n                            \"code\": \"JobNotFound\",\n                            \"message\": \"The specified job aerial-finetune-pretrain_1547432376313 is not found\"\n                        }\n                    },\n                    \"message\": \"Service invocation failed!\\r\\nRequest: POST https://sn1-prod.batchai.core.windows.net/subscriptions/952a710c-8d9c-40c1-9fec-f752138cc0b3/resourceGroups/jkamlworkshop/providers/Microsoft.BatchAI/workspaces/jkamlworkshop/experiments/azureml/jobs/aerial-finetune-pretrain_1547432376313/terminate?api-version=2018-05-01\\r\\nStatus Code: 404 NotFound\\r\\nReason Phrase: The specified job aerial-finetune-pretrain_1547432376313 is not found\\r\\nResponse Body: {\\\"error\\\":{\\\"code\\\":\\\"JobNotFound\\\",\\\"message\\\":\\\"The specified job aerial-finetune-pretrain_1547432376313 is not found\\\"}}\",\n                    \"stackTrace\": \"   at Microsoft.MachineLearning.Common.WebApi.Client.ServiceInvoker._MakeRequest(UriBuilder builder, MethodDetails details, CancellationToken cancellationToken, Object[] parameters)\\n   at Polly.Policy.<>c__DisplayClass181_0`1.<<ExecuteAsyncInternal>b__0>d.MoveNext()\\n--- End of stack trace from previous location where exception was thrown ---\\n   at Polly.RetrySyntaxAsync.<>c__DisplayClass25_1.<<WaitAndRetryAsync>b__1>d.MoveNext()\\n--- End of stack trace from previous location where exception was thrown ---\\n   at Polly.Retry.RetryEngine.ImplementationAsync[TResult](Func`3 action, Context context, CancellationToken cancellationToken, IEnumerable`1 shouldRetryExceptionPredicates, IEnumerable`1 shouldRetryResultPredicates, Func`1 policyStateFactory, Boolean continueOnCapturedContext)\\n   at Polly.Retry.RetryEngine.ImplementationAsync[TResult](Func`3 action, Context context, CancellationToken cancellationToken, IEnumerable`1 shouldRetryExceptionPredicates, IEnumerable`1 shouldRetryResultPredicates, Func`1 policyStateFactory, Boolean continueOnCapturedContext)\\n   at Polly.Policy.ExecuteAsyncInternal[TResult](Func`3 action, Context context, CancellationToken cancellationToken, Boolean continueOnCapturedContext)\\n   at Microsoft.MachineLearning.Common.WebApi.Client.ServiceInvoker._Invoke[T](String methodId, Object[] parameters) in /home/vsts/work/1/s/src/azureml-api/src/Common/WebApi.Client/ServiceInvoker.cs:line 162\\n   at Microsoft.MachineLearning.Execution.Services.BatchAiStrategy.CancelRun(StrategyState initialState) in /home/vsts/work/1/s/src/azureml-api/src/Execution/Services/BatchAiStrategy.cs:line 605\",\n                    \"type\": \"Microsoft.MachineLearning.Common.Core.ServiceInvocationException\"\n                },\n                \"message\": \"BatchAI job aerial-finetune-pretrain_1547432376313 on cluster gpu-cluster does not exist\",\n                \"stackTrace\": \"   at Microsoft.MachineLearning.Execution.Services.BatchAiStrategy.CancelRun(StrategyState initialState) in /home/vsts/work/1/s/src/azureml-api/src/Execution/Services/BatchAiStrategy.cs:line 611\\n   at Microsoft.MachineLearning.Execution.EntryPoints.Api.Controllers.ExecutionController.CancelRun(Guid subscriptionId, String resourceGroupName, String workspaceName, String experimentName, RunDetails runDetails) in /home/vsts/work/1/s/src/azureml-api/src/Execution/EntryPoints/Api/Controllers/ExecutionController.cs:line 270\\n   at lambda_method(Closure , Object )\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeActionMethodAsync()\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeNextActionFilterAsync()\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.Rethrow(ActionExecutedContext context)\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.Next(State& next, Scope& scope, Object& state, Boolean& isCompleted)\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeInnerFilterAsync()\\n   at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.InvokeNextExceptionFilterAsync()\",\n                \"type\": \"Microsoft.MachineLearning.Common.WebApi.Exceptions.BadRequestException\"\n            },\n            \"message\": \"BatchAI job aerial-finetune-pretrain_1547432376313 on cluster gpu-cluster does not exist\"\n        }\n    },\n    \"status_code\": 400,\n    \"url\": \"https://southcentralus.experiments.azureml.net/execution/v1.0/subscriptions/952a710c-8d9c-40c1-9fec-f752138cc0b3/resourceGroups/jkamlworkshop/providers/Microsoft.MachineLearningServices/workspaces/jkamlworkshop/experiments/aerial-finetune-pretrain/cancel\"\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExperimentExecutionException\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-e4e9d818db2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/azureml/core/script_run.py\u001b[0m in \u001b[0;36mcancel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_commands\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0m_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_project_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/azureml/_execution/_commands.py\u001b[0m in \u001b[0;36mcancel\u001b[0;34m(project_object, run_config_object, run_id)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_session_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0m_raise_request_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Canceling run\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/azureml/_execution/_commands.py\u001b[0m in \u001b[0;36m_raise_request_error\u001b[0;34m(response, action)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;31m# response.text is a JSON from execution service.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0mresponse_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_http_exception_response_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mExperimentExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mExperimentExecutionException\u001b[0m: {\n    \"error_details\": {\n        \"correlation\": {\n            \"operation\": \"42cc789eb553c941ba9a44d47437c419\",\n            \"request\": \"5LAT7DLlykQ=\"\n        },\n        \"error\": {\n            \"code\": \"UserError\",\n            \"debugInfo\": {\n                \"innerException\": {\n                    \"errorResponse\": {\n                        \"error\": {\n                            \"code\": \"JobNotFound\",\n                            \"message\": \"The specified job aerial-finetune-pretrain_1547432376313 is not found\"\n                        }\n                    },\n                    \"message\": \"Service invocation failed!\\r\\nRequest: POST https://sn1-prod.batchai.core.windows.net/subscriptions/952a710c-8d9c-40c1-9fec-f752138cc0b3/resourceGroups/jkamlworkshop/providers/Microsoft.BatchAI/workspaces/jkamlworkshop/experiments/azureml/jobs/aerial-finetune-pretrain_1547432376313/terminate?api-version=2018-05-01\\r\\nStatus Code: 404 NotFound\\r\\nReason Phrase: The specified job aerial-finetune-pretrain_1547432376313 is not found\\r\\nResponse Body: {\\\"error\\\":{\\\"code\\\":\\\"JobNotFound\\\",\\\"message\\\":\\\"The specified job aerial-finetune-pretrain_1547432376313 is not found\\\"}}\",\n                    \"stackTrace\": \"   at Microsoft.MachineLearning.Common.WebApi.Client.ServiceInvoker._MakeRequest(UriBuilder builder, MethodDetails details, CancellationToken cancellationToken, Object[] parameters)\\n   at Polly.Policy.<>c__DisplayClass181_0`1.<<ExecuteAsyncInternal>b__0>d.MoveNext()\\n--- End of stack trace from previous location where exception was thrown ---\\n   at Polly.RetrySyntaxAsync.<>c__DisplayClass25_1.<<WaitAndRetryAsync>b__1>d.MoveNext()\\n--- End of stack trace from previous location where exception was thrown ---\\n   at Polly.Retry.RetryEngine.ImplementationAsync[TResult](Func`3 action, Context context, CancellationToken cancellationToken, IEnumerable`1 shouldRetryExceptionPredicates, IEnumerable`1 shouldRetryResultPredicates, Func`1 policyStateFactory, Boolean continueOnCapturedContext)\\n   at Polly.Retry.RetryEngine.ImplementationAsync[TResult](Func`3 action, Context context, CancellationToken cancellationToken, IEnumerable`1 shouldRetryExceptionPredicates, IEnumerable`1 shouldRetryResultPredicates, Func`1 policyStateFactory, Boolean continueOnCapturedContext)\\n   at Polly.Policy.ExecuteAsyncInternal[TResult](Func`3 action, Context context, CancellationToken cancellationToken, Boolean continueOnCapturedContext)\\n   at Microsoft.MachineLearning.Common.WebApi.Client.ServiceInvoker._Invoke[T](String methodId, Object[] parameters) in /home/vsts/work/1/s/src/azureml-api/src/Common/WebApi.Client/ServiceInvoker.cs:line 162\\n   at Microsoft.MachineLearning.Execution.Services.BatchAiStrategy.CancelRun(StrategyState initialState) in /home/vsts/work/1/s/src/azureml-api/src/Execution/Services/BatchAiStrategy.cs:line 605\",\n                    \"type\": \"Microsoft.MachineLearning.Common.Core.ServiceInvocationException\"\n                },\n                \"message\": \"BatchAI job aerial-finetune-pretrain_1547432376313 on cluster gpu-cluster does not exist\",\n                \"stackTrace\": \"   at Microsoft.MachineLearning.Execution.Services.BatchAiStrategy.CancelRun(StrategyState initialState) in /home/vsts/work/1/s/src/azureml-api/src/Execution/Services/BatchAiStrategy.cs:line 611\\n   at Microsoft.MachineLearning.Execution.EntryPoints.Api.Controllers.ExecutionController.CancelRun(Guid subscriptionId, String resourceGroupName, String workspaceName, String experimentName, RunDetails runDetails) in /home/vsts/work/1/s/src/azureml-api/src/Execution/EntryPoints/Api/Controllers/ExecutionController.cs:line 270\\n   at lambda_method(Closure , Object )\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeActionMethodAsync()\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeNextActionFilterAsync()\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.Rethrow(ActionExecutedContext context)\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.Next(State& next, Scope& scope, Object& state, Boolean& isCompleted)\\n   at Microsoft.AspNetCore.Mvc.Internal.ControllerActionInvoker.InvokeInnerFilterAsync()\\n   at Microsoft.AspNetCore.Mvc.Internal.ResourceInvoker.InvokeNextExceptionFilterAsync()\",\n                \"type\": \"Microsoft.MachineLearning.Common.WebApi.Exceptions.BadRequestException\"\n            },\n            \"message\": \"BatchAI job aerial-finetune-pretrain_1547432376313 on cluster gpu-cluster does not exist\"\n        }\n    },\n    \"status_code\": 400,\n    \"url\": \"https://southcentralus.experiments.azureml.net/execution/v1.0/subscriptions/952a710c-8d9c-40c1-9fec-f752138cc0b3/resourceGroups/jkamlworkshop/providers/Microsoft.MachineLearningServices/workspaces/jkamlworkshop/experiments/aerial-finetune-pretrain/cancel\"\n}"
     ]
    }
   ],
   "source": [
    "run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azureml-logs/20_image_build_log.txt', 'azureml-logs/60_control_log.txt', 'azureml-logs/80_driver_log.txt', 'logs/events.out.tfevents.1547433810.6f973db609e94d7a8de1d120e7507365000000', 'outputs/aerial_model_weights_pretrain.h5', 'outputs/aerial_model_pretrain.h5', 'driver_log', 'azureml-logs/azureml.log', 'azureml-logs/55_batchai_execution.txt']\n"
     ]
    }
   ],
   "source": [
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file('outputs//aerial_model_weights_pretrain.h5', '/tmp/models/aerial_model_weights_pretrain.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aerial_model_weights_pretrain.h5\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "ls /tmp/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload weights to the default datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AzureBlob jkamlworstoragedirzndtd azureml-blobstore-5635fa3a-71d5-4d1b-bc80-b0847d6f842b\n",
      "Uploading /tmp/models/aerial_model_weights_pretrain.h5\n",
      "Uploaded /tmp/models/aerial_model_weights_pretrain.h5, 1 files out of an estimated total of 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_a47e131ea7ac4de8964ab140015b6b3d"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the dataset to the DataStore\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "print(ds.datastore_type, ds.account_name, ds.container_name)\n",
    "ds.upload(src_dir='/tmp/models', target_path='models', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "\n",
    "#### Create training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./script/fine-tune.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/fine-tune.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, Input\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.applications import vgg16\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "\n",
    "def custom_classifier(input_shape=(224,224,3), units=256, classes=6,  l1=0.01, l2=0.01, optimizer='adadelta'):\n",
    "    # Create a base vgg16 model\n",
    "    base_model = vgg16.VGG16(\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        pooling='avg')\n",
    "    # Add new top\n",
    "    x = base_model.output\n",
    "    x = Dense(units, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    y = Dense(classes, activation='softmax', kernel_regularizer=l1_l2(l1=l1, l2=l2))(x)\n",
    "    model = Model(inputs=base_model.inputs, outputs=y)\n",
    "    \n",
    "    return model, base_model\n",
    "    \n",
    "        \n",
    "def main(argv=None):\n",
    "    \n",
    "    # get hold of the current run\n",
    "    run = Run.get_submitted_run()\n",
    "    \n",
    "    print(\"Loading data from:\", FLAGS.data_folder)\n",
    "    # Create training and validation data generators\n",
    "    train_data_dir = os.path.join(FLAGS.data_folder, 'train')\n",
    "    valid_data_dir = os.path.join(FLAGS.data_folder, 'valid')\n",
    "    \n",
    "    # A hack to mitigate a bug in TF.Keras 1.12\n",
    "    def preprocess_input_new(x):\n",
    "        img = vgg16.preprocess_input(image.img_to_array(x))\n",
    "        return image.array_to_img(img)\n",
    "    \n",
    "    batchsize=64\n",
    "    classes = [\"Barren\", \"Cultivated\", \"Developed\", \"Forest\", \"Herbaceous\", \"Shrub\"]\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_new)\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        directory=train_data_dir,\n",
    "        target_size=(224, 224),\n",
    "        classes=classes,\n",
    "        batch_size=batchsize)\n",
    "\n",
    "    valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_new)\n",
    "    valid_generator = train_datagen.flow_from_directory(\n",
    "        directory=valid_data_dir,\n",
    "        target_size=(224, 224),\n",
    "        classes=classes,\n",
    "        batch_size=batchsize)\n",
    "    \n",
    "    print(len(train_generator))\n",
    "    print(len(valid_generator))\n",
    "    \n",
    "    \n",
    "    # Create a custom model\n",
    "    model, base_model = custom_classifier()\n",
    "    \n",
    "    weights_file = os.path.join(FLAGS.weights_folder, FLAGS.weights_filename)\n",
    "    model.load_weights(weights_file)\n",
    "    \n",
    "    # Make last convolutional layer trainable\n",
    "    for layer in base_model.layers[:14]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    for layer in base_model.layers[14:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # For fine tuning use SGD with a low learning rate\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [tf.keras.callbacks.TensorBoard(log_dir='./logs')]\n",
    "\n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=FLAGS.epochs,\n",
    "        validation_data=valid_generator,\n",
    "        validation_steps=len(valid_generator))\n",
    "    \n",
    "    # Save the trained model to outputs which is a standard folder expected by AML\n",
    "    print(\"Training completed.\")\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    model_file = os.path.join('outputs', 'aerial_model_finetune.h5')\n",
    "    weights_file = os.path.join('outputs', 'aerial_model_weights_finetune.h5')\n",
    "    print(\"Saving model to: {0}\".format(model_file))\n",
    "    model.save(model_file)\n",
    "    print(\"Saving model weights to: {0}\".format(weights_file))\n",
    "    model.save_weights(weights_file)\n",
    " \n",
    "\n",
    "# Default global parameters\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('batch_size', 32, \"Number of images per batch\")\n",
    "tf.app.flags.DEFINE_integer('epochs', 10, \"Number of epochs to train\")\n",
    "tf.app.flags.DEFINE_integer('units', 512, \"Number of epochs to train\")\n",
    "tf.app.flags.DEFINE_string('data_folder', 'aerialsmall', \"Folder with images\")\n",
    "tf.app.flags.DEFINE_string('weights_folder', 'models', \"Folder with model weights\")\n",
    "tf.app.flags.DEFINE_string('weights_filename', 'aerial_model_weights_pretrain.h5', \"Folder with model weights\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AML Experiment\n",
    "We will track runs of the feature extraction script in a dedicated Experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'aerial-finetune-train'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a fine-tuning training on  the cluster\n",
    "\n",
    "Due to time limitations of the lab, we will run pre-training for 2 epochs only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "script_params = {\n",
    "    '--data_folder': ds.path('aerialsmall').as_mount(),\n",
    "    '--weights_folder': ds.path('models').as_download(),\n",
    "    '--epochs': 3\n",
    "}\n",
    "\n",
    "pip_packages = ['h5py', 'pillow', 'scipy', 'tensorflow-gpu==1.12']\n",
    "\n",
    "est = TensorFlow(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='fine-tune.py',\n",
    "                #node_count=1,\n",
    "                #process_count_per_node=1,\n",
    "                use_gpu=True,\n",
    "                pip_packages=pip_packages\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>aerial-finetune-train</td><td>aerial-finetune-train_1547434178173</td><td>azureml.scriptrun</td><td>Queued</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/952a710c-8d9c-40c1-9fec-f752138cc0b3/resourceGroups/jkamlworkshop/providers/Microsoft.MachineLearningServices/workspaces/jkamlworkshop/experiments/aerial-finetune-train/runs/aerial-finetune-train_1547434178173\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: aerial-finetune-train,\n",
       "Id: aerial-finetune-train_1547434178173,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Queued)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = {\"Run Type\": \"Top pre-train\"}\n",
    "run = exp.submit(est, tags=tags)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5801dfaa449541febeea5c437da8cce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Horovod script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/fine-tune.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, Input\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.applications import vgg16\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "\n",
    "def custom_classifier(input_shape=(224,224,3), units=256, classes=6,  l1=0.01, l2=0.01, optimizer='adadelta'):\n",
    "    # Create a base vgg16 model\n",
    "    base_model = vgg16.VGG16(\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        pooling='avg')\n",
    "    # Add new top\n",
    "    x = base_model.output\n",
    "    x = Dense(units, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    y = Dense(classes, activation='softmax', kernel_regularizer=l1_l2(l1=l1, l2=l2))(x)\n",
    "    model = Model(inputs=base_model.inputs, outputs=y)\n",
    "    \n",
    "    return model, base_model\n",
    "    \n",
    "        \n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# Default global parameters\n",
    "tf.app.flags.DEFINE_integer('batch_size', 32, \"Number of images per batch\")\n",
    "tf.app.flags.DEFINE_integer('epochs', 10, \"Number of epochs to train\")\n",
    "tf.app.flags.DEFINE_integer('units', 512, \"Number of epochs to train\")\n",
    "tf.app.flags.DEFINE_string('data_folder', 'aerialsmall', \"Folder with images\")\n",
    "tf.app.flags.DEFINE_string('weights_folder', 'models', \"Folder with model weights\")\n",
    "tf.app.flags.DEFINE_string('weights_filename', 'aerial_model_weights_pretrain.h5', \"Folder with model weights\")\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    \n",
    "    # get hold of the current run\n",
    "    run = Run.get_submitted_run()\n",
    "    \n",
    "    print(\"Loading data from:\", FLAGS.data_folder)\n",
    "    # Create training and validation data generators\n",
    "    train_data_dir = os.path.join(FLAGS.data_folder, 'train')\n",
    "    valid_data_dir = os.path.join(FLAGS.data_folder, 'valid')\n",
    "    \n",
    "    # A hack to mitigate a bug in TF.Keras 1.12\n",
    "    def preprocess_input_new(x):\n",
    "        img = vgg16.preprocess_input(image.img_to_array(x))\n",
    "        return image.array_to_img(img)\n",
    "    \n",
    "    batchsize=64\n",
    "    classes = [\"Barren\", \"Cultivated\", \"Developed\", \"Forest\", \"Herbaceous\", \"Shrub\"]\n",
    "    \n",
    "    train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_new)\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        directory=train_data_dir,\n",
    "        target_size=(224, 224),\n",
    "        classes=classes,\n",
    "        batch_size=batchsize)\n",
    "\n",
    "    valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_new)\n",
    "    valid_generator = train_datagen.flow_from_directory(\n",
    "        directory=valid_data_dir,\n",
    "        target_size=(224, 224),\n",
    "        classes=classes,\n",
    "        batch_size=batchsize)\n",
    "    \n",
    "    print(len(train_generator))\n",
    "    print(len(valid_generator))\n",
    "    \n",
    "    \n",
    "    # Create a custom model\n",
    "    model, base_model = custom_classifier()\n",
    "    \n",
    "    weights_file = os.path.join(FLAGS.weights_folder, FLAGS.weights_filename)\n",
    "    model.load_weights(weights_file)\n",
    "    \n",
    "    # Make last convolutional layer trainable\n",
    "    for layer in base_model.layers[:14]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    for layer in base_model.layers[14:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # For fine tuning use SGD with a low learning rate\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=FLAGS.epochs,\n",
    "        validation_data=valid_generator,\n",
    "        validation_steps=len(valid_generator))\n",
    "    \n",
    "    # Save the trained model to outputs which is a standard folder expected by AML\n",
    "    #print(\"Training completed.\")\n",
    "    #os.makedirs('outputs', exist_ok=True)\n",
    "    #model_file = os.path.join('outputs', 'aerial_model_pretrain.hd5')\n",
    "    #weights_file = os.path.join('outputs', 'aerial_model_weights_pretrain.hd5')\n",
    "    #print(\"Saving model to: {0}\".format(model_file))\n",
    "    #model.save(model_file, save_format='h5')\n",
    "    #print(\"Saving model weights to: {0}\".format(weights_file))\n",
    "    #model.save_weights(weights_file, save_format='h5')\n",
    " \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Horovod Estimator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "script_params = {\n",
    "    '--data_folder': ds.path('aerialsmall').as_mount(),\n",
    "    '--weights_folder': ds.path('models').as_download(),\n",
    "    '--epochs': 2\n",
    "}\n",
    "\n",
    "pip_packages = ['h5py', 'pillow', 'scipy', 'tensorflow-gpu==1.12']\n",
    "\n",
    "est = TensorFlow(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='fine-tune.py',\n",
    "                node_count=2,\n",
    "                process_count_per_node=1,\n",
    "                use_gpu=True,\n",
    "                pip_packages=pip_packages,\n",
    "                distributed_backend='mpi'\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {\"Run Type\": \"Top pre-train\"}\n",
    "run = exp.submit(est, tags=tags)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
