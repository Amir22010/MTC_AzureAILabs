{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel training using DistributedStrategy\n",
    "\n",
    "\n",
    "# Time series prediction with LSTM (IOT Data)\n",
    "\n",
    "\n",
    "[Solar power forecasting](https://en.wikipedia.org/wiki/Solar_power_forecasting) is a challenging and important problem. The solar energy generation forecasting problem is closely linked to the problem of weather variables forecasting. Indeed, this problem is usually split into two parts, on one hand focusing on the forecasting of solar PV or any other meteorological variable and on the other hand estimating the amount of energy that a concrete power plant will produce with the estimated meteorological resource. In general, the way to deal with this difficult problem is usually related to the spatial and temporal scales we are interested in. This tutorial focusses on a simplified forecasting model using previously generated data from solar panel to predict the future. \n",
    "\n",
    "Using historic daily production of a solar panel, we want to predict the total power production of the solar panel array for a day. We will be using the LSTM based time series prediction model to predict the daily output of a solar panel based on the initial readings of the day. \n",
    "\n",
    "We train the model with historical data of the solar panel. In our example we want to predict the total power production of the solar panel array for the day starting with the initial readings of the day. We start predicting after the first 2 readings and adjust the prediction with each new reading.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.8\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /data/home/demouser/notebooks/aml_config/config.json\n",
      "jkamlworkshop\n",
      "jkamlworkshop\n",
      "southcentralus\n",
      "952a710c-8d9c-40c1-9fec-f752138cc0b3\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Our solar panel, emits two measures at every 30 min interval:\n",
    "- `solar.current` is the current production in Watt\n",
    "- `solar.total` is the total produced for the day so far \n",
    "\n",
    "Our prediction approach involves starting with the first 2 initial readings of the day. Based on these readings we start predicting and adjust the prediction with each new reading. The training data we are going to use comes as a csv file and has the following format:\n",
    "\n",
    ">```\n",
    "time,solar.current,solar.total\n",
    "7am,6.3,1.7\n",
    "7:30am,44.3,11.4\n",
    "...\n",
    ">```\n",
    "\n",
    "The training dataset we use contains data captured for 3 years.\n",
    "The dataset is not pre-processed: it is raw data and contains smaller gaps and errors (like a panel failed to report).\n",
    "\n",
    "#### Pre-processing\n",
    "\n",
    "`generate_solar_data()` function performs the following tasks:\n",
    "- read raw data into a pandas dataframe, \n",
    "- normalize the data, \n",
    "- groups by day and \n",
    "- append the columns \"solar.current.max\" and \"solar.total.max\", and\n",
    "- generates the sequences for each day.\n",
    "\n",
    "*Sequence generation*: All sequences are concatenated into a single list of sequences. There is *no more notion of timestamp* in our train input and **only** the sequences matter.\n",
    "\n",
    "**Note** if we have less than 8 datapoints for a day we skip over the day assuming something is missing in the raw data. If we get more than 14 data points in a day we truncate the readings.\n",
    "\n",
    "#### Training / Testing / Validation data preparation\n",
    "We split the dataset in the following manner: pick in sequence, 8 values for training, 1 for validation and 1 for test until there is no more data. This will spread training, validation and test datasets across the full timeline while preserving time order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>solar.current</th>\n",
       "      <th>solar.total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-12-01 07:00:00</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>1.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-12-01 07:30:00</td>\n",
       "      <td>44.299999</td>\n",
       "      <td>11.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-12-01 08:00:00</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>67.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-12-01 08:30:00</td>\n",
       "      <td>482.000000</td>\n",
       "      <td>250.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-12-01 09:00:00</td>\n",
       "      <td>774.000000</td>\n",
       "      <td>573.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time  solar.current  solar.total\n",
       "0  2013-12-01 07:00:00       6.300000         1.69\n",
       "1  2013-12-01 07:30:00      44.299999        11.36\n",
       "2  2013-12-01 08:00:00     208.000000        67.50\n",
       "3  2013-12-01 08:30:00     482.000000       250.50\n",
       "4  2013-12-01 09:00:00     774.000000       573.50"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/solar.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_solar_data(dataset_path, time_steps, normalize=1, val_size=0.1, test_size=0.1):\n",
    "    \"\"\"\n",
    "    generate sequences to feed to rnn based on data frame with solar panel data\n",
    "    the csv has the format: time ,solar.current, solar.total\n",
    "     (solar.current is the current output in Watt, solar.total is the total production\n",
    "      for the day so far in Watt hours)\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(dataset_path, index_col=\"time\", parse_dates=['time'], dtype=np.float32)\n",
    "    \n",
    "    df[\"date\"] = df.index.date\n",
    "    \n",
    "    # normalize data\n",
    "    df['solar.current'] /= normalize\n",
    "    df['solar.total'] /= normalize\n",
    "    \n",
    "    # group by day, find the max for a day and add a new column .max\n",
    "    grouped = df.groupby(df.index.date).max()\n",
    "    grouped.columns = [\"solar.current.max\", \"solar.total.max\", \"date\"]\n",
    "\n",
    "    # merge continuous readings and daily max values into a single frame\n",
    "    df_merged = pd.merge(df, grouped, right_index=True, on=\"date\")\n",
    "    df_merged = df_merged[[\"solar.current\", \"solar.total\",\n",
    "                           \"solar.current.max\", \"solar.total.max\"]]\n",
    "    # we group by day so we can process a day at a time.\n",
    "    grouped = df_merged.groupby(df_merged.index.date)\n",
    "    per_day = []\n",
    "    for _, group in grouped:\n",
    "        per_day.append(group)\n",
    "\n",
    "    # split the dataset into train, validatation and test sets on day boundaries\n",
    "    val_size = int(len(per_day) * val_size)\n",
    "    test_size = int(len(per_day) * test_size)\n",
    "    next_val = 0\n",
    "    next_test = 0\n",
    "\n",
    "    result_x = {\"train\": [], \"val\": [], \"test\": []}\n",
    "    result_y = {\"train\": [], \"val\": [], \"test\": []}    \n",
    "\n",
    "    # generate sequences a day at a time\n",
    "    for i, day in enumerate(per_day):\n",
    "        # if we have less than 8 datapoints for a day we skip over the\n",
    "        # day assuming something is missing in the raw data\n",
    "        total = day[\"solar.total\"].values.tolist()\n",
    "        if len(total) < 8:\n",
    "            continue\n",
    "        if i >= next_val:\n",
    "            current_set = \"val\"\n",
    "            next_val = i + int(len(per_day) / val_size)\n",
    "        elif i >= next_test:\n",
    "            current_set = \"test\"\n",
    "            next_test = i + int(len(per_day) / test_size)\n",
    "        else:\n",
    "            current_set = \"train\"\n",
    "        max_total_for_day = np.array(day[\"solar.total.max\"].values[0])\n",
    "        for j in range(2, len(total)):\n",
    "            result_x[current_set].append(total[0:j])\n",
    "            result_y[current_set].append([max_total_for_day])\n",
    "            if j >= time_steps:\n",
    "                break\n",
    "                \n",
    "    # pad result_x sequences with 0 and convert to numpy.array\n",
    "    # make result_y a numpy array\n",
    "    for ds in [\"train\", \"val\", \"test\"]:\n",
    "        sequences = pad_sequences(result_x[ds], dtype='float32')\n",
    "        sequences = sequences.reshape((len(sequences), TIMESTEPS, 1))\n",
    "        result_x[ds] = sequences\n",
    "        result_y[ds] = np.array(result_y[ds])\n",
    "    return result_x, result_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We keep up to 14 readings from a day\n",
    "TIMESTEPS = 14\n",
    "\n",
    "# 20000 is the maximum total output in our dataset. We normalize all values with this so our inputs are between 0.0 and 1.0 range\n",
    "\n",
    "NORMALIZE = 20000\n",
    "\n",
    "X, Y = generate_solar_data('data/solar.csv', TIMESTEPS, normalize=NORMALIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10747, 14, 1)\n",
      "[[[0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.0006985]]\n",
      "\n",
      " [[0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.0006985]\n",
      "  [0.0033175]]\n",
      "\n",
      " [[0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.       ]\n",
      "  [0.0006985]\n",
      "  [0.0033175]\n",
      "  [0.010375 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(X['train'].shape)\n",
    "print(X['train'][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10747, 1)\n",
      "[[0.239]\n",
      " [0.239]\n",
      " [0.239]]\n"
     ]
    }
   ],
   "source": [
    "print(Y['train'].shape)\n",
    "print(Y['train'][0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the training data to the default datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Save training to HDF5 file\n",
    "output_file = '/tmp/train_iot.h5'\n",
    "with h5py.File(output_file, \"w\") as hfile:\n",
    "     features_dset = hfile.create_dataset('features', data=X['train'])\n",
    "     labels_dset = hfile.create_dataset('labels', data=Y['train'])\n",
    "\n",
    "# Save validation to HDF5 file\n",
    "output_file = '/tmp/valid_iot.h5'\n",
    "with h5py.File(output_file, \"w\") as hfile:\n",
    "     features_dset = hfile.create_dataset('features', data=X['val'])\n",
    "     labels_dset = hfile.create_dataset('labels', data=Y['val'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_iot.h5  valid_iot.h5  wli_674bac5b-8413-44db-a017-b2e810be4238\r\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "print(ds.datastore_type, ds.account_name, ds.container_name)\n",
    "\n",
    "ds.upload(src_dir='/tmp/datasets/aerialsmall', target_path='aerialsmall', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM network\n",
    "\n",
    "Training LSTM is computationally intensive. We will use remote compute target for training.\n",
    "\n",
    "Corresponding  to the maximum possible 14 data points in the input sequence, we model our network with 14 LSTM cells, 1 cell for each data point we take during the day. \n",
    "\n",
    "The outputs from the LSTMs are fed into a dense layer and we randomly dropout 20% of the values to not overfit the model to the training set. The output of the dense layer becomes the prediction our model generates.\n",
    "\n",
    "Notice: We lost the timestamp altogether; in our model only the sequences of readings matter. \n",
    "\n",
    "Our LSTM model has the following design:\n",
    "![lstm](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/lstm.png)\n",
    "\n",
    "The network model is an exact translation of the network diagram above.\n",
    "\n",
    "### Define a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Add\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers import LSTM\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "\n",
    "def network_model(hidden_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(hidden_size, return_sequences=False, input_shape=(TIMESTEPS, 1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    return model\n",
    "\n",
    "hidden_size = 15\n",
    "optimizer = 'rmsprop'\n",
    "loss = 'mse'\n",
    "metrics = ['mse']\n",
    "\n",
    "model = network_model(hidden_size)\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from datetime import datetime\n",
    "\n",
    "batch_size = 14\n",
    "epochs = 50\n",
    "\n",
    "now = datetime.now().strftime(\"%d-%m-%y-%H-%M\")\n",
    "logdir = \"../../../logdir/{0}\".format(now)\n",
    "modelpath = \"../../../SaveModel/model-{0}.h5\".format(now)\n",
    "tensorboard = TensorBoard(log_dir=logdir)\n",
    "run_history = model.fit(X['train'], Y['train'], batch_size=batch_size, epochs=epochs, validation_data = (X['val'], Y['val']), callbacks = [tensorboard])\n",
    "model.save(modelpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "model = load_model(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(X['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(result)\n",
    "plt.plot(Y['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
