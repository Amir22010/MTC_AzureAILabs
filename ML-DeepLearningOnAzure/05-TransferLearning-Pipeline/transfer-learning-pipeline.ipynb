{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Custom Image Classification Model\n",
    "## Azure Machine Learning Pipelines version\n",
    "\n",
    "In this lab, you will developed a custom image classification model to automatically classify the type of land shown in aerial images of 224-meter x 224-meter plots. Land use classification models can be used to track urbanization, deforestation, loss of wetlands, and other major environmental trends using periodically collected aerial imagery. The images used in this lab are based off of imagery from the U.S. National Land Cover Database. U.S. National Land Cover Database defines six primary classes of land use: *Developed*, *Barren*, *Forested*, *Grassland*, *Shrub*, *Cultivated*. Example images from each land use class are shown here:\n",
    "\n",
    "Developed | Cultivated | Barren\n",
    "--------- | ------ | ----------\n",
    "![Developed](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/developed1.png) | ![Cultivated](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/cultivated1.png) | ![Barren](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/barren1.png)\n",
    "\n",
    "Forested | Grassland | Shrub\n",
    "---------| ----------| -----\n",
    "![Forested](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/forest1.png) | ![Grassland](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/grassland1.png) | ![Shrub](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/shrub1.png)\n",
    "\n",
    "You shall employ a machine learning technique called transfer learning. Transfer learning is one of the fastest (code and run-time-wise) ways to start using deep learning. It allows for the reuse of knowledge gained while solving one problem to a different but related problem. For example, knowledge gained while learning to recognize landmarks and landscapes could apply when trying to recognize aerial land plots. Transfer Learning makes it feasible to train very effective ML models on relatively small training data sets.\n",
    "\n",
    "Although the primary goal of this lab is to understand how to use Azure ML to orchestrate deep learning workflows rather then to dive into Deep Learning techniques, ask the instructor if you want to better understand the approach utilized in the lab.\n",
    "\n",
    "You will start by pre-processing training images into a set of powerful features - sometimes referred to as bottleneck features.\n",
    "\n",
    "To create bottleneck features you will utilize a pre-trained Deep Learning network that was trained on a general computer vision domain. \n",
    "\n",
    "Although, the pre-trained network does not know how to classify aerial land plot images, it knows enough about representing image concepts that if we use it to pre-process aerial images, the extracted image features can be used to effectively train a relatively simple classifier on a **limited number** of samples.\n",
    "\n",
    "The below diagram represents the architecture of our solution.\n",
    "\n",
    "![Transfer Learning](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/TLArch.png)\n",
    "\n",
    "We will use **ResNet50** trained on **imagenet** dataset to extract features. We will occasionally refer to this component of the solution as a featurizer. The output of the featurizer is a vector of 2048 floating point numbers, each representing a feature extracted from an image. \n",
    "\n",
    "We will then use extracted features to train a simple fully connected neural network (the top) that will peform final image classification.\n",
    "\n",
    "To orcherstrate this workflow, we will use *Azure Machine Learning Pipelines*. Our pipeline will have 3 steps:\n",
    "\n",
    "1. Copy training images from a public Azure Blob Storage container to the default storage in AML Workspace\n",
    "2. Extract features \n",
    "3. Train the top network\n",
    "\n",
    "**NOTE: Please make sure to update Azure ML SDK to the latest version.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to AML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Azure ML Managed Compute\n",
    "\n",
    "To run the lab's scripts we will utilize Azure ML managed compute resources. Specifically, an autoscale cluster of *Standard_NC6* VMs (equipped with Tesla K80 GPU). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"gpu-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 1)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_NC6\")\n",
    "#vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_ND6s\")\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "     # For a more detailed view of current AmlCompute status, use the 'status' property    \n",
    "    print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Copy Step\n",
    "\n",
    "We will use the AML's built-in **DataTransferStep** to copy training images from the remote container to the default storage in the Workspace. \n",
    "\n",
    "**Step** is a unit of execution in Azure ML Pipelines. **Step** typically needs a target of execution (compute target), a script to execute, and may require script arguments and inputs, and can produce outputs. The step also could take a number of other parameters. Azure Machine Learning Pipelines provides the following built-in Steps:\n",
    "\n",
    "- PythonScriptStep: Add a step to run a Python script in a Pipeline.\n",
    "- AdlaStep: Adds a step to run U-SQL script using Azure Data Lake Analytics.\n",
    "- DataTransferStep: Transfers data between Azure Blob and Data Lake accounts.\n",
    "- DatabricksStep: Adds a DataBricks notebook as a step in a Pipeline.\n",
    "- HyperDriveStep: Creates a Hyper Drive step for Hyper Parameter Tuning in a Pipeline.\n",
    "\n",
    "**DataTransferStep** is used to transfer data between Azure Blob, Azure Data Lake Store, and Azure SQL database.\n",
    "\n",
    "To configure **DataTransferStep** you need to provide:\n",
    "\n",
    "- **name:** Name of module\n",
    "- **source_data_reference:** Input connection that serves as source of data transfer operation.\n",
    "- **destination_data_reference:** Input connection that serves as destination of data transfer operation.\n",
    "- **compute_target:** Azure Data Factory to use for transferring data.\n",
    "- **allow_reuse:** Whether the step should reuse results of previous DataTransferStep when run with same inputs. Set as False to force data to be transferred again.\n",
    "\n",
    "Optional arguments to explicitly specify whether a path corresponds to a file or a directory. These are useful when storage contains both file and directory with the same name or when creating a new destination path.\n",
    "\n",
    "- **source_reference_type:** An optional string specifying the type of source_data_reference. Possible values include: 'file', 'directory'. When not specified, we use the type of existing path or directory if it's a new path.\n",
    "- **destination_reference_type:** An optional string specifying the type of destination_data_reference. Possible values include: 'file', 'directory'. When not specified, we use the type of existing path or directory if it's a new path.\n",
    "\n",
    "The **DataTransferStep** utilizes Azure Data Factory as a processing engine.\n",
    "\n",
    "### Register input Datastore\n",
    "\n",
    "This is where the training images are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "# Create input data store\n",
    "images_account = 'azureailabs'\n",
    "images_container = 'aerialsmall'\n",
    "datastore_name = 'input_data'\n",
    "SAS_TOKEN=\"?sv=2018-03-28&ss=b&srt=co&sp=rdl&se=2019-12-31T08:42:04Z&st=2019-02-07T00:42:04Z&spr=https&sig=a1PW3b6%2FNvWWBo3m8luTQkdbyj%2FZW%2FBJDR9RFpjUf%2BQ%3D\"\n",
    "\n",
    "# Check if the datastore exists. If not create a new one\n",
    "try:\n",
    "    input_ds = Datastore.get(ws, datastore_name)\n",
    "    print('Found existing datastore for input images:', input_ds.name)\n",
    "except:\n",
    "    input_ds = Datastore.register_azure_blob_container(workspace=ws, datastore_name=datastore_name,\n",
    "                                            container_name=images_container,\n",
    "                                            account_name=images_account,\n",
    "                                            sas_token=SAS_TOKEN)\n",
    "    print('Creating new datastore for input images')\n",
    "    \n",
    "print(input_ds.name, input_ds.datastore_type, input_ds.account_name, input_ds.container_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the default Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_ds = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"Blobstore's name: {}\".format(default_ds.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataReferences\n",
    "\n",
    "Data references point to specific locations within datastores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "remote_data_ref = DataReference(\n",
    "    datastore=input_ds,\n",
    "    data_reference_name=\"remote_container\",\n",
    "    path_on_datastore=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_data_ref = DataReference(\n",
    "    datastore=default_ds,\n",
    "    data_reference_name=\"input_images\",\n",
    "    path_on_datastore=\"input_images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Data Factory Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import DataFactoryCompute\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "\n",
    "\n",
    "data_factory_name = 'jkmlwsadf'\n",
    "\n",
    "def get_or_create_data_factory(workspace, factory_name):\n",
    "    try:\n",
    "        return DataFactoryCompute(workspace, factory_name)\n",
    "    except ComputeTargetException as e:\n",
    "        if 'ComputeTargetNotFound' in e.message:\n",
    "            print('Data factory not found, creating...')\n",
    "            provisioning_config = DataFactoryCompute.provisioning_configuration()\n",
    "            data_factory = ComputeTarget.create(workspace, factory_name, provisioning_config)\n",
    "            data_factory.wait_for_completion()\n",
    "            return data_factory\n",
    "        else:\n",
    "            raise e\n",
    "            \n",
    "data_factory_compute = get_or_create_data_factory(ws, data_factory_name)\n",
    "\n",
    "print(\"setup data factory account complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataTransferStep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import DataTransferStep\n",
    "\n",
    "transfer_images_step = DataTransferStep(\n",
    "    name=\"transfer_images\",\n",
    "    source_data_reference=remote_data_ref,\n",
    "    destination_data_reference=images_data_ref,\n",
    "    source_reference_type='directory',\n",
    "    destination_reference_type='directory',\n",
    "    allow_reuse=False,\n",
    "    compute_target=data_factory_compute)\n",
    "\n",
    "print(\"data transfer step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**. At this point, there does not seem to be a way (at least it is not well ocumented) of sequentially linking DataTranserStep with PythonScriptStep in the same Pipeline. As such we will execute this step in a discrete Pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[transfer_images_step])\n",
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "experiment_name = 'aerial-transfer-pipeline'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "pipeline_run = exp.submit(pipeline)\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_factory_compute.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction Step\n",
    "\n",
    "As noted in the introduction, we will utilize a pretrained **ResNet50** convolutional neural net as a featurizer. This CNN was trained on *Imagenet* dataset. \n",
    "\n",
    "We will encapsulate feature extraction as an Azure Machine Learning Pipeline *PythonScriptStep*.\n",
    "\n",
    "The following code will create the *PythonScriptStep* to be executed in the Azure Machine Learning Compute using code in the `extract.py` script.\n",
    "\n",
    "The script processes an input image datasets into an output bottleneck feature sets. The script expects the images to be organized in the below folder structure:\n",
    "```\n",
    "Barren/\n",
    "Cultivated/\n",
    "Developed/\n",
    "Forest/\n",
    "Herbaceous/\n",
    "Shrub/\n",
    "```\n",
    "\n",
    "The location of the input dataset and the location where to save the output dataset are passed to the script as command line parameters. The output dataset will be stored in a binary HDF5 data format used commonly in Machine Learning and High Performance Computing solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a feature extraction script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/extract.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import resnet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def create_bottleneck_features(data_dir, output_file, featurizer):\n",
    "    \n",
    "    # A hack to mitigate a bug in TF.Keras 1.12\n",
    "    def preprocess_input_new(x):\n",
    "        img = resnet50.preprocess_input(image.img_to_array(x))\n",
    "        return image.array_to_img(img)\n",
    "    \n",
    "    # Create a Keras generator to read and pre-process images. The generator\n",
    "    # will return batches of numpy arrays representing pre-processed images\n",
    "    batchsize=64\n",
    "    classes = [\"Barren\", \"Cultivated\", \"Developed\", \"Forest\", \"Herbaceous\", \"Shrub\"]\n",
    "    datagen = ImageDataGenerator(preprocessing_function=preprocess_input_new)\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory=data_dir,\n",
    "        target_size=(224, 224),\n",
    "        classes=classes,\n",
    "        batch_size=batchsize)\n",
    "\n",
    "    # Generate bottleneck features\n",
    "    # Due to the bug in Tensorflow 1.12 we cannot use predict_generator \n",
    "    # Instead we are invoking model.predict in an explicit loop\n",
    "    features = []\n",
    "    labels = []\n",
    "    batches = len(generator)\n",
    "    for i in range(batches):\n",
    "        image_batch, label_batch = generator.next()\n",
    "        features.extend(featurizer.predict(image_batch, batch_size=batchsize))\n",
    "        labels.extend(label_batch)\n",
    "        print(\"Processed batch: {} out of {}\".format(i+1, batches))\n",
    "        \n",
    "    features = np.asarray(features)\n",
    "    labels = np.asarray(labels)   \n",
    " \n",
    "    # Save dataset to HDF5 file\n",
    "    print(\"Saving features to {}\".format(output_file))\n",
    "    print(\"   Features: \", features.shape)\n",
    "    print(\"   Labels: \", labels.shape)\n",
    "    with h5py.File(output_file, \"w\") as hfile:\n",
    "        features_dset = hfile.create_dataset('features', data=features)\n",
    "        labels_dset = hfile.create_dataset('labels', data=labels)\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# Default global parameters\n",
    "tf.app.flags.DEFINE_integer('batch_size', 64, \"Number of images per batch\")\n",
    "tf.app.flags.DEFINE_string('data_folder', 'aerialsmall', \"Folder with training and validation images\")\n",
    "tf.app.flags.DEFINE_string('output_data_dir', 'bottleneck', \"A folder for saving bottleneck features\")\n",
    "tf.app.flags.DEFINE_string('output_file_suffix', 'aerial_bottleneck_keras.h5', \"Filename template for output features\")\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    print(\"Starting\")\n",
    "    print(\"Reading training data from:\", FLAGS.data_folder)\n",
    "    print(\"Output bottleneck files will be saved to:\", FLAGS.output_data_dir)\n",
    "    os.makedirs(FLAGS.output_data_dir, exist_ok=True)\n",
    "   \n",
    "    train_data_dir = os.path.join(FLAGS.data_folder, 'train')\n",
    "    valid_data_dir = os.path.join(FLAGS.data_folder, 'valid')\n",
    "    \n",
    "    train_output_file = os.path.join(FLAGS.output_data_dir, 'train_' + FLAGS.output_file_suffix)\n",
    "    valid_output_file = os.path.join(FLAGS.output_data_dir, 'valid_' + FLAGS.output_file_suffix)\n",
    "    \n",
    "    print(train_output_file)\n",
    "    print(valid_output_file)\n",
    "    \n",
    "    # Create a featurizer\n",
    "    featurizer = resnet50.ResNet50(\n",
    "                weights = 'imagenet', \n",
    "                input_shape=(224,224,3), \n",
    "                include_top = False,\n",
    "                pooling = 'avg')\n",
    "\n",
    "    print(\"Creating training bottleneck features\")\n",
    "    create_bottleneck_features(train_data_dir, train_output_file, featurizer)\n",
    "    print(\"Creating validation bottleneck features\")\n",
    "    create_bottleneck_features(valid_data_dir, valid_output_file, featurizer)\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Datastores and DataReferences\n",
    "\n",
    "Our training images are located in the Workspace's default datastore in the `input_images` folder. After the script completes, its output - the bottleneck features file - will be uploaded to the `bottleneck_features` folder in the default datastore.\n",
    "\n",
    "Intermediate data (or output of a PythonScriptStep) is represented by a *PipelineData* object. *PipelineData* can be produced by one step and consumed in another step by providing the PipelineData object as an output of one step and the input of one or more steps.\n",
    "\n",
    "Constructing PipelineData\n",
    "- name: [Required] Name of the data item within the pipeline graph\n",
    "- datastore_name: Name of the Datastore to write this output to\n",
    "- output_name: Name of the output\n",
    "- output_mode: Specifies \"upload\" or \"mount\" modes for producing output (default: mount)\n",
    "- output_path_on_compute: For \"upload\" mode, the path to which the module writes this output during execution\n",
    "- output_overwrite: Flag to overwrite pre-existing data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "# Create output PipelineData object\n",
    "bottleneck_features_ref = PipelineData(\n",
    "    name = \"bottleneck_features\",\n",
    "    datastore=default_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PythonScriptStep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# Configure runtime for script execution\n",
    "run_config = RunConfiguration(framework=\"python\")\n",
    "run_config.environment.docker.enabled = True\n",
    "run_config.environment.docker.gpu_support = True\n",
    "# There seems to be a bug in RunConfiguration. Despite setting gpu_support to true\n",
    "# it is pulling the base CPU image. \n",
    "run_config.environment.docker.base_image = 'mcr.microsoft.com/azureml/base-gpu:0.2.1'\n",
    "\n",
    "pip_packages = ['h5py', \n",
    "                'pillow', \n",
    "                'scipy',\n",
    "                'tensorflow-gpu']\n",
    "\n",
    "conda_dependencies = CondaDependencies.create(pip_packages=pip_packages)\n",
    "run_config.environment.python.conda_dependencies = conda_dependencies\n",
    "\n",
    "# Define command line arguments for the script\n",
    "arguments = [\"--data_folder\", images_data_ref,\n",
    "             \"--output_data_dir\", bottleneck_features_ref]\n",
    "\n",
    "# Create the step\n",
    "extract_step = PythonScriptStep(\n",
    "    script_name='extract.py',\n",
    "    arguments=arguments,\n",
    "    inputs=[images_data_ref],\n",
    "    outputs=[bottleneck_features_ref],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=script_folder,\n",
    "    allow_reuse=True,\n",
    "    runconfig=run_config\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Step\n",
    "\n",
    "Finally, we will define the training step. We are going to use *PythonScriptStep* and reuse the same runtime configuration as for the feature extraction step.\n",
    "\n",
    "### Create the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_name = 'train.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import resnet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten, Input\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "\n",
    "# Define network\n",
    "def fcn_classifier(input_shape=(2048,), units=512, classes=6,  l1=0.01, l2=0.01):\n",
    "    features = Input(shape=input_shape)\n",
    "    x = Dense(units, activation='relu')(features)\n",
    "    x = Dropout(0.5)(x)\n",
    "    y = Dense(classes, activation='softmax', kernel_regularizer=l1_l2(l1=l1, l2=l2))(x)\n",
    "    model = Model(inputs=features, outputs=y)\n",
    "    model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Training regime\n",
    "def train_evaluate():\n",
    "   \n",
    "    print(\"Loading bottleneck features\")\n",
    "    train_file_name = os.path.join(FLAGS.data_folder, FLAGS.train_file_name)\n",
    "    valid_file_name = os.path.join(FLAGS.data_folder, FLAGS.valid_file_name)\n",
    "    \n",
    "    # Load bottleneck training features and labels\n",
    "    with h5py.File(train_file_name, \"r\") as hfile:\n",
    "        X_train = np.array(hfile.get('features'))\n",
    "        y_train = np.array(hfile.get('labels'))\n",
    "        \n",
    "     # Load bottleneck validation features and labels\n",
    "    with h5py.File(valid_file_name, \"r\") as hfile:\n",
    "        X_validation = np.array(hfile.get('features'))\n",
    "        y_validation = np.array(hfile.get('labels'))\n",
    "        \n",
    "             \n",
    "    print(y_train.shape)\n",
    "    print(y_validation.shape)\n",
    "    \n",
    "    # Create a network\n",
    "    model = fcn_classifier(input_shape=(2048,), units=FLAGS.units, l1=FLAGS.l1, l2=FLAGS.l2)\n",
    "    \n",
    "    \n",
    "    # Start training\n",
    "    print(\"Starting training\")\n",
    "    model.fit(X_train, y_train,\n",
    "          batch_size=FLAGS.batch_size,\n",
    "          epochs=FLAGS.epochs,\n",
    "          shuffle=True,\n",
    "          validation_data=(X_validation, y_validation))\n",
    "          \n",
    "    # Save the trained model to outputs which is a standard folder expected by AML\n",
    "    print(\"Training completed.\")\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    model_file = os.path.join('outputs', 'aerial_fcnn_classifier.h5')\n",
    "    print(\"Saving model to: {0}\".format(model_file))\n",
    "    model.save(model_file)\n",
    "    \n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# Default global parameters\n",
    "tf.app.flags.DEFINE_integer('batch_size', 32, \"Number of images per batch\")\n",
    "tf.app.flags.DEFINE_integer('epochs', 10, \"Number of epochs to train\")\n",
    "tf.app.flags.DEFINE_integer('units', 512, \"Number of epochs to train\")\n",
    "tf.app.flags.DEFINE_float('l1', 0.01, \"l1 regularization\")\n",
    "tf.app.flags.DEFINE_float('l2', 0.01, \"l2 regularization\")\n",
    "tf.app.flags.DEFINE_string('data_folder', './bottleneck', \"Folder with bottleneck features and labels\")\n",
    "tf.app.flags.DEFINE_string('train_file_name', 'train_aerial_bottleneck_keras.h5', \"Training file name\")\n",
    "tf.app.flags.DEFINE_string('valid_file_name', 'valid_aerial_bottleneck_keras.h5', \"Validation file name\")\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    \n",
    "    train_evaluate()\n",
    "  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training step\n",
    "\n",
    "Notice that we are using the output of the `extract_step` as the input to the `train_step`. By doing that we are creating a pipeline with Data Dependency. As a result the `train_step` will not start till the `extract_step` is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define command line arguments for the script\n",
    "arguments = [\"--data_folder\", bottleneck_features_ref,\n",
    "             '--train_file_name', 'train_aerial_bottleneck_keras.h5',\n",
    "             '--valid_file_name', 'valid_aerial_bottleneck_keras.h5',\n",
    "             '--epochs', 50]\n",
    "\n",
    "# Create the step\n",
    "train_step = PythonScriptStep(\n",
    "    script_name='train.py',\n",
    "    arguments=arguments,\n",
    "    inputs=[bottleneck_features_ref],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=script_folder,\n",
    "    allow_reuse=True,\n",
    "    runconfig=run_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the pipeline\n",
    "\n",
    "Since there are Data Dependencies between the `extraction` and `training` steps we only need to specify the last step (`training`) in the pipeline's configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[train_step])\n",
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start and monitor a remote run\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "experiment_name = 'aerial-classifier-training-pipeline'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "\n",
    "pipeline_run = exp.submit(pipeline)\n",
    "RunDetails(pipeline_run).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.get_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "If you are not going to walk through the other labs, delete the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
