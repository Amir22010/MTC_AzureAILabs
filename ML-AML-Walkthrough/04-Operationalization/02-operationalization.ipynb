{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab 2 - Model Deplyoment \n",
    "\n",
    "In this lab you will deploy a trained model to containers using an Azure Container Instance and and Azure Kubernetes Service using the Azure Machine Learning SDK.\n",
    "\n",
    "![AML Arch](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/amlarch.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify AML SDK Installed\n",
    "# view version history at https://pypi.org/project/azureml-sdk/#history \n",
    "import azureml.core\n",
    "print(\"SDK Version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# Read the workspace config from file\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the lab datasets\n",
    "\n",
    "The datasets have been downloaded in Lab 1 into the local folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a temporary folder to store locally relevant content for this notebook\n",
    "datasetsFolderName = '../datasets'\n",
    "# os.makedirs(datasetsFolderName, exist_ok=True)\n",
    "# print('Content files will be saved to {0}'.format(datasetsFolderName))\n",
    "\n",
    "# filesToDownload = ['UsedCars_Clean.csv', 'UsedCars_Affordability.csv']\n",
    "\n",
    "# for fileToDownload in filesToDownload:\n",
    "#  downloadCommand = 'wget -O ''{0}/{1}'' ''https://databricksdemostore.blob.core.windows.net/data/aml-labs/{1}'''.format(datasetsFolderName, fileToDownload)\n",
    "#  print(downloadCommand)\n",
    "#  os.system(downloadCommand)\n",
    "  \n",
    "# List the downloaded files\n",
    "os.listdir(datasetsFolderName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a simple model locally and register it with *Model Registry*\n",
    "\n",
    "This lab builds upon the lessons learned in the previous lab, but is designed to be self contained. As such we will repeat model training. After training is completed, the trained model will be registered in *Model Registry*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper method that will train, score and register the classifier using different settings\n",
    "import numpy as np\n",
    "\n",
    "from azureml.core.model import Model \n",
    "from azureml.core.run import Run\n",
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "from sklearn import linear_model \n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "\n",
    "def train_eval_register_model(ws, experiment_name, model_name, full_X, full_Y,training_set_percentage):\n",
    "\n",
    "    # start a training run by defining an experiment\n",
    "    myexperiment = Experiment(ws, experiment_name)\n",
    "    run = myexperiment.start_logging()\n",
    "\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(full_X, full_Y, train_size=training_set_percentage, random_state=42)\n",
    "\n",
    "    # Flatten labels\n",
    "    train_Y = np.ravel(train_Y)\n",
    "    test_Y = np.ravel(test_Y)\n",
    "    \n",
    "    # Convert to float\n",
    "    train_X = train_X.astype(float)\n",
    "    test_X = test_X.astype(float)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(train_X)\n",
    "    clf = linear_model.LogisticRegression(C=1)\n",
    "    clf.fit(X_scaled, train_Y)\n",
    "\n",
    "    scaled_inputs = scaler.transform(test_X)\n",
    "    predictions = clf.predict(scaled_inputs)\n",
    "    score = accuracy_score(test_Y, predictions)\n",
    "\n",
    "    print(\"With %0.2f percent of data, model accuracy reached %0.4f.\" % (training_set_percentage, score))\n",
    "\n",
    "    # Log the training metrics to Azure Machine Learning service run history\n",
    "    run.log(\"Training_Set_Percentage\", training_set_percentage)\n",
    "    run.log(\"Accuracy\", score)\n",
    "\n",
    "    # Serialize the model to a pickle file in the outputs folder\n",
    "    output_model_path = 'outputs/' + model_name + '.pkl'\n",
    "    pickle.dump(clf,open(output_model_path,'wb'))\n",
    "    print('Exported model to ', output_model_path)\n",
    "\n",
    "    # Serialize the scaler as a pickle file in the same folder as the model\n",
    "    output_scaler_path = 'outputs/' + 'scaler' + '.pkl'\n",
    "    pickle.dump(scaler,open(output_scaler_path,'wb'))\n",
    "    print('Exported scaler to ', output_scaler_path)\n",
    "\n",
    "    # notice for the model_path, we supply the name of the outputs folder without a trailing slash\n",
    "    # this will ensure both the model and the scaler get uploaded.\n",
    "    registered_model = Model.register(model_path='outputs', model_name=model_name, workspace=ws)\n",
    "\n",
    "    print(registered_model.name, registered_model.id, registered_model.version, sep = '\\t')\n",
    "\n",
    "    run.complete()\n",
    "\n",
    "    return (registered_model, clf, scaler, score, run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment and Run training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load our training data set\n",
    "df_affordability = pd.read_csv('../datasets/UsedCars_Affordability.csv', delimiter=',')\n",
    "\n",
    "full_X = df_affordability[[\"Age\", \"KM\"]]\n",
    "full_Y = df_affordability[[\"Affordable\"]]\n",
    "\n",
    "# Create an experiment, log metrics and register the created model\n",
    "experiment_name = \"usedcars_training_deployment\"\n",
    "model_name = \"usedcarsmodel\"\n",
    "training_set_percentage = 0.50\n",
    "registered_model, model, scaler, score, run = train_eval_register_model(ws, \n",
    "                                                                        experiment_name, \n",
    "                                                                        model_name, \n",
    "                                                                        full_X, full_Y, \n",
    "                                                                        training_set_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and test version of a model from Azure Machine Learning.\n",
    "\n",
    "Download the model registered in the previous step and run a quick test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model to a local directory\n",
    "model_name = 'usedcarsmodel'\n",
    "scaler_name = 'scaler'\n",
    "\n",
    "model_path = Model.get_model_path(model_name, _workspace=ws)\n",
    "\n",
    "# Re-load the model\n",
    "model = pickle.load(open(os.path.join(model_path, model_name + '.pkl'), 'rb'))\n",
    "print(\"Loaded model from:\", os.path.join(model_path, model_name + '.pkl'))\n",
    "\n",
    "# Re-load the scaler\n",
    "scaler = pickle.load(open(os.path.join(model_path, scaler_name + '.pkl'),'rb'))\n",
    "print(\"Loaded scaler from:\", os.path.join(model_path, scaler_name + '.pkl'))\n",
    "\n",
    "# Run a quick test\n",
    "age = 60\n",
    "km = 40000\n",
    "\n",
    "scaled_input = scaler.transform([[age, km]])\n",
    "prediction = model.predict(scaled_input)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and deploy the container image encapsulating the model\n",
    "\n",
    "When you deploy a model using AML to either ACI or AKS, you are deploying a Docker container encapsulating a trained model, its dependencies, and a web services wrapper around the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Conda dependencies environment file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "mycondaenv = CondaDependencies.create(conda_packages=['scikit-learn','numpy','pandas'])\n",
    "\n",
    "with open(\"mydeployenv.yml\",\"w\") as f:\n",
    "    f.write(mycondaenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the content of 'yml' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mydeployenv.yml\",\"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create scoring script.\n",
    "With Azure Machine Learning, you have full control over the logic of the webservice which includes how it loads your model, transforms web service inputs, uses the model for scoring and returns the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model \n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from azureml.core import Run\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.run import Run\n",
    "from azureml.core.experiment import Experiment\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def init():\n",
    "    try:\n",
    "        # One-time initialization of predictive model and scaler\n",
    "        from azureml.core.model import Model\n",
    "        \n",
    "        global trainedModel   \n",
    "        global scaler\n",
    "\n",
    "        model_name = \"usedcarsmodel\" \n",
    "        model_path = Model.get_model_path(model_name)\n",
    "        print('Looking for models in: ', model_path)\n",
    "\n",
    "        trainedModel = pickle.load(open(os.path.join(model_path,'usedcarsmodel.pkl'), 'rb'))\n",
    "        \n",
    "        scaler = pickle.load(open(os.path.join(model_path,'scaler.pkl'),'rb'))\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Exception during init: ', str(e))\n",
    "\n",
    "def run(input_json):     \n",
    "    try:\n",
    "        inputs = json.loads(input_json)\n",
    "\n",
    "        #Scale the input\n",
    "        scaled_input = scaler.transform(inputs)\n",
    "        \n",
    "        #Get the scored result\n",
    "        prediction = json.dumps(trainedModel.predict(scaled_input).tolist())\n",
    "\n",
    "    except Exception as e:\n",
    "        prediction = str(e)\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create docker image for deployment\n",
    "\n",
    "To create a Container Image, you need four things: the model metadata (as retrieved from Model Registry), the scoring script file, the runtime configuration (defining whether Python or PySpark should be used) and the Conda Dependencies file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.image import ContainerImage, Image\n",
    "\n",
    "# Retrieve the model metadata from Model Registry\n",
    "model_name = 'usedcarsmodel'\n",
    "model = Model(workspace=ws, name=model_name)\n",
    "\n",
    "# Define runtime\n",
    "runtime = \"python\" \n",
    "\n",
    "# Define scoring script\n",
    "driver_file = \"score.py\"\n",
    "\n",
    "# Define conda dependencies\n",
    "conda_file = \"mydeployenv.yml\"\n",
    "\n",
    "# configure the image\n",
    "image_config = ContainerImage.image_configuration(execution_script=driver_file, \n",
    "                                                  runtime=runtime, \n",
    "                                                  conda_file=conda_file,\n",
    "                                                  description=\"Image for used cars predictor\",\n",
    "                                                  tags={\"Classifier\": \"Logistic regression\"})\n",
    "\n",
    "image = Image.create(name = \"used-car-classifier-image\",\n",
    "                     models = [model],\n",
    "                     image_config = image_config, \n",
    "                     workspace = ws)\n",
    "\n",
    "image.wait_for_creation(show_output = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the container image to ACI\n",
    "\n",
    "With the Container Image  in hand, you are almost ready to deploy to ACI. The next step is to define the size of the VM that ACI will use to run your Container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "aci_config = AciWebservice.deploy_configuration(\n",
    "    cpu_cores = 1, \n",
    "    memory_gb = 1, \n",
    "    tags = {'name':'Azure ML ACI'}, \n",
    "    description = 'This is a deployment of the affordibility predictor.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you can deploy the image to the webservice to ACI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "service_name = \"usedcarsmlservice-aci\"\n",
    "print(\"Deploying: \", service_name)\n",
    "aci_service = Webservice.deploy_from_image(deployment_config = aci_config,\n",
    "                                           image = image,\n",
    "                                           name = service_name,\n",
    "                                           workspace = ws)\n",
    "aci_service.wait_for_deployment(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the service\n",
    "\n",
    "Once the webservice deployment completes, you can use the returned webservice object to invoke the webservice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "age = 60\n",
    "km = 40000\n",
    "test_data  = json.dumps([[age,km]])\n",
    "test_data\n",
    "result = aci_service.run(input_data=test_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the container image to AKS\n",
    "\n",
    "Once you are familiar with the process for deploying a webservice to ACI, you will find the process for deploying to AKS to be similar with one additional step that creates the AKS cluster first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provision an AKS cluster \n",
    "\n",
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "\n",
    "# Use the default configuration, overriding the default location to a known region that supports AKS\n",
    "prov_config = AksCompute.provisioning_configuration(location='westus2')\n",
    "\n",
    "aks_name = 'aks-cluster01' \n",
    "\n",
    "# Create the cluster\n",
    "aks_target = ComputeTarget.create(workspace = ws, \n",
    "                                  name = aks_name, \n",
    "                                  provisioning_configuration = prov_config)\n",
    "\n",
    "\n",
    "# Wait for cluster to be ready\n",
    "aks_target.wait_for_completion(show_output = True)\n",
    "print(aks_target.provisioning_state)\n",
    "print(aks_target.provisioning_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your AKS cluster ready, now you can deploy your webservice. Once again, you need to provide a configuration for the size of resources allocated from the AKS cluster to run instances of your Container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the web service configuration (using defaults)\n",
    "aks_config = AksWebservice.deploy_configuration()\n",
    "\n",
    "aks_service_name ='usedcarsmlservice-aks'\n",
    "\n",
    "aks_service = Webservice.deploy_from_image(\n",
    "  workspace=ws, \n",
    "  name=aks_service_name, \n",
    "  image = image,\n",
    "  deployment_target=aks_target\n",
    "  )\n",
    "\n",
    "\n",
    "aks_service.wait_for_deployment(show_output = True)\n",
    "print(aks_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the service\n",
    "As before, you can use the webservice object returned by the deploy_from_model method to invoke your deployed webservice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "age = 60\n",
    "km = 40000\n",
    "test_data  = json.dumps([[age,km]])\n",
    "test_data\n",
    "result = aks_service.run(input_data=test_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Make sure to remove ACI and AKS deployments. Use Azure Portal to remove *Deployments* and *AKS Compute*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
